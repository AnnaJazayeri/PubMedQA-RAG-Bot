{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ce1716a-5e10-48a6-a869-326c824ef979",
   "metadata": {},
   "source": [
    "### Project: Question/Answering system using PubMedQA dataset and microsoft/BiomedNLP-PubMedBERT... pretrained model\n",
    "## Anna jazayeri 2916723\n",
    "## Data mining CIS 660"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb87fe85-3b22-4c40-9c6b-723af61f4126",
   "metadata": {},
   "source": [
    "# 1. Setup and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48bb34a5-a354-4bf2-91d1-8c4a5385e4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "\n",
    "!pip install -q datasets transformers accelerate\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d9ddaec-ffe9-40d5-a63c-4da1af226d1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device and seeds\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864d595e-7e42-4707-ba8b-dd96ef001a92",
   "metadata": {},
   "source": [
    "# 2. Data prep: build a big unlabeled (question, context) dataset\n",
    "This uses labeled + unlabeled + artificial PubMedQA configs, but we ignore all labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c4be047-c580-4448-828f-79414765f522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(273518, 273518)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Load PubMedQA splits and extract question/context pairs\n",
    "\n",
    "def extract_qc_pairs(hf_dataset_split):\n",
    "    questions = []\n",
    "    contexts = []\n",
    "    for ex in hf_dataset_split:\n",
    "        q = ex[\"question\"]\n",
    "        ctx_list = ex[\"context\"][\"contexts\"]\n",
    "        ctx = \" \".join(ctx_list)\n",
    "        questions.append(q)\n",
    "        contexts.append(ctx)\n",
    "    return questions, contexts\n",
    "\n",
    "# Load the three official configs; comment any out if you want\n",
    "ds_labeled = load_dataset(\"pubmed_qa\", \"pqa_labeled\")[\"train\"]\n",
    "ds_unlab = load_dataset(\"pubmed_qa\", \"pqa_unlabeled\")[\"train\"]\n",
    "ds_artificial = load_dataset(\"pubmed_qa\", \"pqa_artificial\")[\"train\"]\n",
    "\n",
    "q_l, c_l = extract_qc_pairs(ds_labeled)\n",
    "q_u, c_u = extract_qc_pairs(ds_unlab)\n",
    "q_a, c_a = extract_qc_pairs(ds_artificial)\n",
    "\n",
    "all_questions = q_l + q_u + q_a\n",
    "all_contexts = c_l + c_u + c_a\n",
    "\n",
    "len(all_questions), len(all_contexts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee703d3d-c6d1-4867-851c-f83515e2bcca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                            question  \\\n",
       " 0  Do mitochondria play a role in remodelling lac...   \n",
       " 1  Landolt C and snellen e acuity: differences in...   \n",
       " 2  Syncope during bathing in infants, a pediatric...   \n",
       " 3  Are the long-term results of the transanal pul...   \n",
       " 4  Can tailored interventions increase mammograph...   \n",
       " \n",
       "                                              context  id  \n",
       " 0  Programmed cell death (PCD) is the regulated d...   0  \n",
       " 1  Assessment of visual acuity depends on the opt...   1  \n",
       " 2  Apparent life-threatening events in infants ar...   2  \n",
       " 3  The transanal endorectal pull-through (TERPT) ...   3  \n",
       " 4  Telephone counseling and tailored print commun...   4  ,\n",
       " 273467)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.1 Build a single DataFrame and deduplicate\n",
    "\n",
    "df_all = pd.DataFrame({\n",
    "    \"question\": all_questions,\n",
    "    \"context\": all_contexts\n",
    "})\n",
    "\n",
    "# Optional: drop exact duplicate (question, context) pairs\n",
    "df_all = df_all.drop_duplicates(subset=[\"question\", \"context\"]).reset_index(drop=True)\n",
    "\n",
    "# Add a global id for retrieval evaluation later\n",
    "df_all[\"id\"] = np.arange(len(df_all))\n",
    "\n",
    "df_all.head(), len(df_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad308db0-fe4c-44b1-bef8-a67619d5a008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(218773, 27347, 27347)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.2 Train/val/test splits (80/10/10)\n",
    "\n",
    "train_df, temp_df = train_test_split(\n",
    "    df_all,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=0.5,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "len(train_df), len(val_df), len(test_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6ccb74-ef5c-46b5-a365-565a38a6292e",
   "metadata": {},
   "source": [
    "# 3. Dataset and DataLoader for dual encoder\n",
    "We’ll have a dataset that returns question + context strings, and a collate_fn that tokenizes both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "949465db-eb5f-43a3-9b08-2b59626cf0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Dataset and collate_fn\n",
    "\n",
    "class QCDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.questions = df[\"question\"].tolist()\n",
    "        self.contexts = df[\"context\"].tolist()\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"question\": self.questions[idx],\n",
    "            \"context\": self.contexts[idx]\n",
    "        }\n",
    "\n",
    "model_name = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "max_length = 128 # you can increase to 256 if you have GPU memory\n",
    "\n",
    "def collate_fn(batch):\n",
    "    qs = [b[\"question\"] for b in batch]\n",
    "    cs = [b[\"context\"] for b in batch]\n",
    "\n",
    "    q_enc = tokenizer(\n",
    "        qs,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    c_enc = tokenizer(\n",
    "        cs,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"q_input_ids\": q_enc[\"input_ids\"],\n",
    "        \"q_attention_mask\": q_enc[\"attention_mask\"],\n",
    "        \"c_input_ids\": c_enc[\"input_ids\"],\n",
    "        \"c_attention_mask\": c_enc[\"attention_mask\"]\n",
    "    }\n",
    "\n",
    "train_dataset = QCDataset(train_df)\n",
    "val_dataset = QCDataset(val_df)\n",
    "\n",
    "batch_size = 8 # adjust based on GPU memory\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06c05b1-df9e-4a6c-9c4a-736f1092a171",
   "metadata": {},
   "source": [
    "# 4. Dual encoder model + InfoNCE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ee92fca-b09e-4f48-aa5d-43a818f4d623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Dual encoder using shared PubMedBERT\n",
    "\n",
    "class DualEncoder(nn.Module):\n",
    "    def __init__(self, encoder_name):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(encoder_name)\n",
    "        self.hidden_size = self.encoder.config.hidden_size\n",
    "\n",
    "    def encode(self, input_ids, attention_mask):\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        token_embs = outputs.last_hidden_state # [batch_size, seq_len, hidden_dim]\n",
    "        cls_emb = token_embs[:, 0, :] # [batch_size, hidden_dim]\n",
    "        cls_emb = cls_emb / cls_emb.norm(p=2, dim=1, keepdim=True).clamp(min=1e-8)\n",
    "        return cls_emb\n",
    "\n",
    "    def forward(self, q_input_ids, q_attention_mask, c_input_ids, c_attention_mask):\n",
    "        q_emb = self.encode(q_input_ids, q_attention_mask)\n",
    "        c_emb = self.encode(c_input_ids, c_attention_mask)\n",
    "        return q_emb, c_emb\n",
    "\n",
    "dual_encoder = DualEncoder(model_name).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80d0eb14-9d10-41a0-8404-3ba54a74fd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 InfoNCE / contrastive loss with in-batch negatives\n",
    "\n",
    "def contrastive_loss(q_emb, c_emb, temperature=0.05):\n",
    "    batch_size = q_emb.size(0)\n",
    "    sim_matrix = q_emb @ c_emb.t()  # [B, B]\n",
    "    sim_matrix = sim_matrix / temperature\n",
    "    targets = torch.arange(batch_size, device=sim_matrix.device)\n",
    "\n",
    "    loss_q2c = F.cross_entropy(sim_matrix, targets)\n",
    "    loss_c2q = F.cross_entropy(sim_matrix.t(), targets)\n",
    "    loss = (loss_q2c + loss_c2q) / 2.0\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c429d4-e9da-4ef8-8222-e892b8dbd0bd",
   "metadata": {},
   "source": [
    "# 5. Training loop (self-supervised)\n",
    "We’ll train just to minimize the contrastive loss; you can limit number of batches/epochs to what your GPU can handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eae19a33-66a8-48b0-8679-4ffec1c5b443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Training setup\n",
    "\n",
    "num_epochs = 2 # start small, we can increase later\n",
    "learning_rate = 2e-5\n",
    "\n",
    "optimizer = torch.optim.AdamW(dual_encoder.parameters(), lr=learning_rate)\n",
    "\n",
    "# Optional: linear warmup + decay scheduler\n",
    "num_update_steps_per_epoch = math.ceil(len(train_loader))\n",
    "max_train_steps = num_epochs * num_update_steps_per_epoch\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.1 * max_train_steps),\n",
    "    num_training_steps=max_train_steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e06bd065-daa0-4f5a-aa37-35b768bce8ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Step 50/27347 | Avg loss: 1.6948\n",
      "Epoch 1 | Step 100/27347 | Avg loss: 1.6557\n",
      "Epoch 1 | Step 150/27347 | Avg loss: 1.5713\n",
      "Epoch 1 | Step 200/27347 | Avg loss: 1.4328\n",
      "Epoch 1 | Step 250/27347 | Avg loss: 1.2528\n",
      "Epoch 1 | Step 300/27347 | Avg loss: 1.0843\n",
      "Epoch 1 | Step 350/27347 | Avg loss: 0.9491\n",
      "Epoch 1 | Step 400/27347 | Avg loss: 0.8409\n",
      "Epoch 1 | Step 450/27347 | Avg loss: 0.7539\n",
      "Epoch 1 | Step 500/27347 | Avg loss: 0.6827\n",
      "Epoch 1 | Step 550/27347 | Avg loss: 0.6245\n",
      "Epoch 1 | Step 600/27347 | Avg loss: 0.5757\n",
      "Epoch 1 | Step 650/27347 | Avg loss: 0.5334\n",
      "Epoch 1 | Step 700/27347 | Avg loss: 0.4972\n",
      "Epoch 1 | Step 750/27347 | Avg loss: 0.4659\n",
      "Epoch 1 | Step 800/27347 | Avg loss: 0.4388\n",
      "Epoch 1 | Step 850/27347 | Avg loss: 0.4139\n",
      "Epoch 1 | Step 900/27347 | Avg loss: 0.3917\n",
      "Epoch 1 | Step 950/27347 | Avg loss: 0.3718\n",
      "Epoch 1 | Step 1000/27347 | Avg loss: 0.3547\n",
      "Epoch 1 | Step 1050/27347 | Avg loss: 0.3382\n",
      "Epoch 1 | Step 1100/27347 | Avg loss: 0.3234\n",
      "Epoch 1 | Step 1150/27347 | Avg loss: 0.3100\n",
      "Epoch 1 | Step 1200/27347 | Avg loss: 0.2976\n",
      "Epoch 1 | Step 1250/27347 | Avg loss: 0.2861\n",
      "Epoch 1 | Step 1300/27347 | Avg loss: 0.2755\n",
      "Epoch 1 | Step 1350/27347 | Avg loss: 0.2655\n",
      "Epoch 1 | Step 1400/27347 | Avg loss: 0.2564\n",
      "Epoch 1 | Step 1450/27347 | Avg loss: 0.2479\n",
      "Epoch 1 | Step 1500/27347 | Avg loss: 0.2398\n",
      "Epoch 1 | Step 1550/27347 | Avg loss: 0.2323\n",
      "Epoch 1 | Step 1600/27347 | Avg loss: 0.2252\n",
      "Epoch 1 | Step 1650/27347 | Avg loss: 0.2185\n",
      "Epoch 1 | Step 1700/27347 | Avg loss: 0.2123\n",
      "Epoch 1 | Step 1750/27347 | Avg loss: 0.2064\n",
      "Epoch 1 | Step 1800/27347 | Avg loss: 0.2012\n",
      "Epoch 1 | Step 1850/27347 | Avg loss: 0.1963\n",
      "Epoch 1 | Step 1900/27347 | Avg loss: 0.1914\n",
      "Epoch 1 | Step 1950/27347 | Avg loss: 0.1866\n",
      "Epoch 1 | Step 2000/27347 | Avg loss: 0.1821\n",
      "Epoch 1 | Step 2050/27347 | Avg loss: 0.1782\n",
      "Epoch 1 | Step 2100/27347 | Avg loss: 0.1741\n",
      "Epoch 1 | Step 2150/27347 | Avg loss: 0.1704\n",
      "Epoch 1 | Step 2200/27347 | Avg loss: 0.1666\n",
      "Epoch 1 | Step 2250/27347 | Avg loss: 0.1630\n",
      "Epoch 1 | Step 2300/27347 | Avg loss: 0.1595\n",
      "Epoch 1 | Step 2350/27347 | Avg loss: 0.1563\n",
      "Epoch 1 | Step 2400/27347 | Avg loss: 0.1531\n",
      "Epoch 1 | Step 2450/27347 | Avg loss: 0.1500\n",
      "Epoch 1 | Step 2500/27347 | Avg loss: 0.1471\n",
      "Epoch 1 | Step 2550/27347 | Avg loss: 0.1443\n",
      "Epoch 1 | Step 2600/27347 | Avg loss: 0.1416\n",
      "Epoch 1 | Step 2650/27347 | Avg loss: 0.1391\n",
      "Epoch 1 | Step 2700/27347 | Avg loss: 0.1366\n",
      "Epoch 1 | Step 2750/27347 | Avg loss: 0.1344\n",
      "Epoch 1 | Step 2800/27347 | Avg loss: 0.1322\n",
      "Epoch 1 | Step 2850/27347 | Avg loss: 0.1299\n",
      "Epoch 1 | Step 2900/27347 | Avg loss: 0.1278\n",
      "Epoch 1 | Step 2950/27347 | Avg loss: 0.1259\n",
      "Epoch 1 | Step 3000/27347 | Avg loss: 0.1239\n",
      "Epoch 1 | Step 3050/27347 | Avg loss: 0.1219\n",
      "Epoch 1 | Step 3100/27347 | Avg loss: 0.1200\n",
      "Epoch 1 | Step 3150/27347 | Avg loss: 0.1182\n",
      "Epoch 1 | Step 3200/27347 | Avg loss: 0.1168\n",
      "Epoch 1 | Step 3250/27347 | Avg loss: 0.1151\n",
      "Epoch 1 | Step 3300/27347 | Avg loss: 0.1137\n",
      "Epoch 1 | Step 3350/27347 | Avg loss: 0.1121\n",
      "Epoch 1 | Step 3400/27347 | Avg loss: 0.1105\n",
      "Epoch 1 | Step 3450/27347 | Avg loss: 0.1090\n",
      "Epoch 1 | Step 3500/27347 | Avg loss: 0.1075\n",
      "Epoch 1 | Step 3550/27347 | Avg loss: 0.1061\n",
      "Epoch 1 | Step 3600/27347 | Avg loss: 0.1047\n",
      "Epoch 1 | Step 3650/27347 | Avg loss: 0.1034\n",
      "Epoch 1 | Step 3700/27347 | Avg loss: 0.1022\n",
      "Epoch 1 | Step 3750/27347 | Avg loss: 0.1008\n",
      "Epoch 1 | Step 3800/27347 | Avg loss: 0.0996\n",
      "Epoch 1 | Step 3850/27347 | Avg loss: 0.0984\n",
      "Epoch 1 | Step 3900/27347 | Avg loss: 0.0973\n",
      "Epoch 1 | Step 3950/27347 | Avg loss: 0.0961\n",
      "Epoch 1 | Step 4000/27347 | Avg loss: 0.0950\n",
      "Epoch 1 | Step 4050/27347 | Avg loss: 0.0938\n",
      "Epoch 1 | Step 4100/27347 | Avg loss: 0.0929\n",
      "Epoch 1 | Step 4150/27347 | Avg loss: 0.0919\n",
      "Epoch 1 | Step 4200/27347 | Avg loss: 0.0910\n",
      "Epoch 1 | Step 4250/27347 | Avg loss: 0.0900\n",
      "Epoch 1 | Step 4300/27347 | Avg loss: 0.0890\n",
      "Epoch 1 | Step 4350/27347 | Avg loss: 0.0881\n",
      "Epoch 1 | Step 4400/27347 | Avg loss: 0.0872\n",
      "Epoch 1 | Step 4450/27347 | Avg loss: 0.0862\n",
      "Epoch 1 | Step 4500/27347 | Avg loss: 0.0853\n",
      "Epoch 1 | Step 4550/27347 | Avg loss: 0.0845\n",
      "Epoch 1 | Step 4600/27347 | Avg loss: 0.0836\n",
      "Epoch 1 | Step 4650/27347 | Avg loss: 0.0827\n",
      "Epoch 1 | Step 4700/27347 | Avg loss: 0.0819\n",
      "Epoch 1 | Step 4750/27347 | Avg loss: 0.0811\n",
      "Epoch 1 | Step 4800/27347 | Avg loss: 0.0804\n",
      "Epoch 1 | Step 4850/27347 | Avg loss: 0.0795\n",
      "Epoch 1 | Step 4900/27347 | Avg loss: 0.0788\n",
      "Epoch 1 | Step 4950/27347 | Avg loss: 0.0780\n",
      "Epoch 1 | Step 5000/27347 | Avg loss: 0.0773\n",
      "Epoch 1 | Step 5050/27347 | Avg loss: 0.0766\n",
      "Epoch 1 | Step 5100/27347 | Avg loss: 0.0759\n",
      "Epoch 1 | Step 5150/27347 | Avg loss: 0.0752\n",
      "Epoch 1 | Step 5200/27347 | Avg loss: 0.0746\n",
      "Epoch 1 | Step 5250/27347 | Avg loss: 0.0740\n",
      "Epoch 1 | Step 5300/27347 | Avg loss: 0.0733\n",
      "Epoch 1 | Step 5350/27347 | Avg loss: 0.0727\n",
      "Epoch 1 | Step 5400/27347 | Avg loss: 0.0722\n",
      "Epoch 1 | Step 5450/27347 | Avg loss: 0.0716\n",
      "Epoch 1 | Step 5500/27347 | Avg loss: 0.0710\n",
      "Epoch 1 | Step 5550/27347 | Avg loss: 0.0704\n",
      "Epoch 1 | Step 5600/27347 | Avg loss: 0.0698\n",
      "Epoch 1 | Step 5650/27347 | Avg loss: 0.0694\n",
      "Epoch 1 | Step 5700/27347 | Avg loss: 0.0689\n",
      "Epoch 1 | Step 5750/27347 | Avg loss: 0.0685\n",
      "Epoch 1 | Step 5800/27347 | Avg loss: 0.0681\n",
      "Epoch 1 | Step 5850/27347 | Avg loss: 0.0675\n",
      "Epoch 1 | Step 5900/27347 | Avg loss: 0.0670\n",
      "Epoch 1 | Step 5950/27347 | Avg loss: 0.0665\n",
      "Epoch 1 | Step 6000/27347 | Avg loss: 0.0660\n",
      "Epoch 1 | Step 6050/27347 | Avg loss: 0.0656\n",
      "Epoch 1 | Step 6100/27347 | Avg loss: 0.0651\n",
      "Epoch 1 | Step 6150/27347 | Avg loss: 0.0647\n",
      "Epoch 1 | Step 6200/27347 | Avg loss: 0.0642\n",
      "Epoch 1 | Step 6250/27347 | Avg loss: 0.0638\n",
      "Epoch 1 | Step 6300/27347 | Avg loss: 0.0634\n",
      "Epoch 1 | Step 6350/27347 | Avg loss: 0.0629\n",
      "Epoch 1 | Step 6400/27347 | Avg loss: 0.0625\n",
      "Epoch 1 | Step 6450/27347 | Avg loss: 0.0620\n",
      "Epoch 1 | Step 6500/27347 | Avg loss: 0.0616\n",
      "Epoch 1 | Step 6550/27347 | Avg loss: 0.0612\n",
      "Epoch 1 | Step 6600/27347 | Avg loss: 0.0607\n",
      "Epoch 1 | Step 6650/27347 | Avg loss: 0.0603\n",
      "Epoch 1 | Step 6700/27347 | Avg loss: 0.0599\n",
      "Epoch 1 | Step 6750/27347 | Avg loss: 0.0595\n",
      "Epoch 1 | Step 6800/27347 | Avg loss: 0.0591\n",
      "Epoch 1 | Step 6850/27347 | Avg loss: 0.0587\n",
      "Epoch 1 | Step 6900/27347 | Avg loss: 0.0583\n",
      "Epoch 1 | Step 6950/27347 | Avg loss: 0.0580\n",
      "Epoch 1 | Step 7000/27347 | Avg loss: 0.0576\n",
      "Epoch 1 | Step 7050/27347 | Avg loss: 0.0572\n",
      "Epoch 1 | Step 7100/27347 | Avg loss: 0.0568\n",
      "Epoch 1 | Step 7150/27347 | Avg loss: 0.0565\n",
      "Epoch 1 | Step 7200/27347 | Avg loss: 0.0561\n",
      "Epoch 1 | Step 7250/27347 | Avg loss: 0.0558\n",
      "Epoch 1 | Step 7300/27347 | Avg loss: 0.0554\n",
      "Epoch 1 | Step 7350/27347 | Avg loss: 0.0551\n",
      "Epoch 1 | Step 7400/27347 | Avg loss: 0.0548\n",
      "Epoch 1 | Step 7450/27347 | Avg loss: 0.0544\n",
      "Epoch 1 | Step 7500/27347 | Avg loss: 0.0541\n",
      "Epoch 1 | Step 7550/27347 | Avg loss: 0.0538\n",
      "Epoch 1 | Step 7600/27347 | Avg loss: 0.0535\n",
      "Epoch 1 | Step 7650/27347 | Avg loss: 0.0531\n",
      "Epoch 1 | Step 7700/27347 | Avg loss: 0.0528\n",
      "Epoch 1 | Step 7750/27347 | Avg loss: 0.0525\n",
      "Epoch 1 | Step 7800/27347 | Avg loss: 0.0523\n",
      "Epoch 1 | Step 7850/27347 | Avg loss: 0.0520\n",
      "Epoch 1 | Step 7900/27347 | Avg loss: 0.0517\n",
      "Epoch 1 | Step 7950/27347 | Avg loss: 0.0514\n",
      "Epoch 1 | Step 8000/27347 | Avg loss: 0.0512\n",
      "Epoch 1 | Step 8050/27347 | Avg loss: 0.0509\n",
      "Epoch 1 | Step 8100/27347 | Avg loss: 0.0507\n",
      "Epoch 1 | Step 8150/27347 | Avg loss: 0.0504\n",
      "Epoch 1 | Step 8200/27347 | Avg loss: 0.0501\n",
      "Epoch 1 | Step 8250/27347 | Avg loss: 0.0498\n",
      "Epoch 1 | Step 8300/27347 | Avg loss: 0.0495\n",
      "Epoch 1 | Step 8350/27347 | Avg loss: 0.0493\n",
      "Epoch 1 | Step 8400/27347 | Avg loss: 0.0490\n",
      "Epoch 1 | Step 8450/27347 | Avg loss: 0.0488\n",
      "Epoch 1 | Step 8500/27347 | Avg loss: 0.0486\n",
      "Epoch 1 | Step 8550/27347 | Avg loss: 0.0484\n",
      "Epoch 1 | Step 8600/27347 | Avg loss: 0.0482\n",
      "Epoch 1 | Step 8650/27347 | Avg loss: 0.0479\n",
      "Epoch 1 | Step 8700/27347 | Avg loss: 0.0476\n",
      "Epoch 1 | Step 8750/27347 | Avg loss: 0.0474\n",
      "Epoch 1 | Step 8800/27347 | Avg loss: 0.0472\n",
      "Epoch 1 | Step 8850/27347 | Avg loss: 0.0469\n",
      "Epoch 1 | Step 8900/27347 | Avg loss: 0.0467\n",
      "Epoch 1 | Step 8950/27347 | Avg loss: 0.0465\n",
      "Epoch 1 | Step 9000/27347 | Avg loss: 0.0462\n",
      "Epoch 1 | Step 9050/27347 | Avg loss: 0.0461\n",
      "Epoch 1 | Step 9100/27347 | Avg loss: 0.0459\n",
      "Epoch 1 | Step 9150/27347 | Avg loss: 0.0457\n",
      "Epoch 1 | Step 9200/27347 | Avg loss: 0.0455\n",
      "Epoch 1 | Step 9250/27347 | Avg loss: 0.0453\n",
      "Epoch 1 | Step 9300/27347 | Avg loss: 0.0452\n",
      "Epoch 1 | Step 9350/27347 | Avg loss: 0.0450\n",
      "Epoch 1 | Step 9400/27347 | Avg loss: 0.0448\n",
      "Epoch 1 | Step 9450/27347 | Avg loss: 0.0446\n",
      "Epoch 1 | Step 9500/27347 | Avg loss: 0.0444\n",
      "Epoch 1 | Step 9550/27347 | Avg loss: 0.0442\n",
      "Epoch 1 | Step 9600/27347 | Avg loss: 0.0440\n",
      "Epoch 1 | Step 9650/27347 | Avg loss: 0.0439\n",
      "Epoch 1 | Step 9700/27347 | Avg loss: 0.0437\n",
      "Epoch 1 | Step 9750/27347 | Avg loss: 0.0436\n",
      "Epoch 1 | Step 9800/27347 | Avg loss: 0.0434\n",
      "Epoch 1 | Step 9850/27347 | Avg loss: 0.0432\n",
      "Epoch 1 | Step 9900/27347 | Avg loss: 0.0430\n",
      "Epoch 1 | Step 9950/27347 | Avg loss: 0.0428\n",
      "Epoch 1 | Step 10000/27347 | Avg loss: 0.0427\n",
      "Epoch 1 | Step 10050/27347 | Avg loss: 0.0425\n",
      "Epoch 1 | Step 10100/27347 | Avg loss: 0.0423\n",
      "Epoch 1 | Step 10150/27347 | Avg loss: 0.0421\n",
      "Epoch 1 | Step 10200/27347 | Avg loss: 0.0420\n",
      "Epoch 1 | Step 10250/27347 | Avg loss: 0.0418\n",
      "Epoch 1 | Step 10300/27347 | Avg loss: 0.0416\n",
      "Epoch 1 | Step 10350/27347 | Avg loss: 0.0414\n",
      "Epoch 1 | Step 10400/27347 | Avg loss: 0.0412\n",
      "Epoch 1 | Step 10450/27347 | Avg loss: 0.0411\n",
      "Epoch 1 | Step 10500/27347 | Avg loss: 0.0410\n",
      "Epoch 1 | Step 10550/27347 | Avg loss: 0.0408\n",
      "Epoch 1 | Step 10600/27347 | Avg loss: 0.0407\n",
      "Epoch 1 | Step 10650/27347 | Avg loss: 0.0406\n",
      "Epoch 1 | Step 10700/27347 | Avg loss: 0.0404\n",
      "Epoch 1 | Step 10750/27347 | Avg loss: 0.0403\n",
      "Epoch 1 | Step 10800/27347 | Avg loss: 0.0401\n",
      "Epoch 1 | Step 10850/27347 | Avg loss: 0.0400\n",
      "Epoch 1 | Step 10900/27347 | Avg loss: 0.0398\n",
      "Epoch 1 | Step 10950/27347 | Avg loss: 0.0397\n",
      "Epoch 1 | Step 11000/27347 | Avg loss: 0.0395\n",
      "Epoch 1 | Step 11050/27347 | Avg loss: 0.0394\n",
      "Epoch 1 | Step 11100/27347 | Avg loss: 0.0392\n",
      "Epoch 1 | Step 11150/27347 | Avg loss: 0.0391\n",
      "Epoch 1 | Step 11200/27347 | Avg loss: 0.0389\n",
      "Epoch 1 | Step 11250/27347 | Avg loss: 0.0388\n",
      "Epoch 1 | Step 11300/27347 | Avg loss: 0.0387\n",
      "Epoch 1 | Step 11350/27347 | Avg loss: 0.0385\n",
      "Epoch 1 | Step 11400/27347 | Avg loss: 0.0384\n",
      "Epoch 1 | Step 11450/27347 | Avg loss: 0.0383\n",
      "Epoch 1 | Step 11500/27347 | Avg loss: 0.0381\n",
      "Epoch 1 | Step 11550/27347 | Avg loss: 0.0380\n",
      "Epoch 1 | Step 11600/27347 | Avg loss: 0.0378\n",
      "Epoch 1 | Step 11650/27347 | Avg loss: 0.0377\n",
      "Epoch 1 | Step 11700/27347 | Avg loss: 0.0376\n",
      "Epoch 1 | Step 11750/27347 | Avg loss: 0.0375\n",
      "Epoch 1 | Step 11800/27347 | Avg loss: 0.0373\n",
      "Epoch 1 | Step 11850/27347 | Avg loss: 0.0372\n",
      "Epoch 1 | Step 11900/27347 | Avg loss: 0.0370\n",
      "Epoch 1 | Step 11950/27347 | Avg loss: 0.0369\n",
      "Epoch 1 | Step 12000/27347 | Avg loss: 0.0368\n",
      "Epoch 1 | Step 12050/27347 | Avg loss: 0.0366\n",
      "Epoch 1 | Step 12100/27347 | Avg loss: 0.0365\n",
      "Epoch 1 | Step 12150/27347 | Avg loss: 0.0364\n",
      "Epoch 1 | Step 12200/27347 | Avg loss: 0.0363\n",
      "Epoch 1 | Step 12250/27347 | Avg loss: 0.0362\n",
      "Epoch 1 | Step 12300/27347 | Avg loss: 0.0361\n",
      "Epoch 1 | Step 12350/27347 | Avg loss: 0.0360\n",
      "Epoch 1 | Step 12400/27347 | Avg loss: 0.0358\n",
      "Epoch 1 | Step 12450/27347 | Avg loss: 0.0357\n",
      "Epoch 1 | Step 12500/27347 | Avg loss: 0.0356\n",
      "Epoch 1 | Step 12550/27347 | Avg loss: 0.0354\n",
      "Epoch 1 | Step 12600/27347 | Avg loss: 0.0353\n",
      "Epoch 1 | Step 12650/27347 | Avg loss: 0.0352\n",
      "Epoch 1 | Step 12700/27347 | Avg loss: 0.0351\n",
      "Epoch 1 | Step 12750/27347 | Avg loss: 0.0350\n",
      "Epoch 1 | Step 12800/27347 | Avg loss: 0.0348\n",
      "Epoch 1 | Step 12850/27347 | Avg loss: 0.0347\n",
      "Epoch 1 | Step 12900/27347 | Avg loss: 0.0346\n",
      "Epoch 1 | Step 12950/27347 | Avg loss: 0.0345\n",
      "Epoch 1 | Step 13000/27347 | Avg loss: 0.0344\n",
      "Epoch 1 | Step 13050/27347 | Avg loss: 0.0343\n",
      "Epoch 1 | Step 13100/27347 | Avg loss: 0.0342\n",
      "Epoch 1 | Step 13150/27347 | Avg loss: 0.0341\n",
      "Epoch 1 | Step 13200/27347 | Avg loss: 0.0340\n",
      "Epoch 1 | Step 13250/27347 | Avg loss: 0.0339\n",
      "Epoch 1 | Step 13300/27347 | Avg loss: 0.0338\n",
      "Epoch 1 | Step 13350/27347 | Avg loss: 0.0337\n",
      "Epoch 1 | Step 13400/27347 | Avg loss: 0.0336\n",
      "Epoch 1 | Step 13450/27347 | Avg loss: 0.0335\n",
      "Epoch 1 | Step 13500/27347 | Avg loss: 0.0334\n",
      "Epoch 1 | Step 13550/27347 | Avg loss: 0.0333\n",
      "Epoch 1 | Step 13600/27347 | Avg loss: 0.0332\n",
      "Epoch 1 | Step 13650/27347 | Avg loss: 0.0331\n",
      "Epoch 1 | Step 13700/27347 | Avg loss: 0.0330\n",
      "Epoch 1 | Step 13750/27347 | Avg loss: 0.0329\n",
      "Epoch 1 | Step 13800/27347 | Avg loss: 0.0328\n",
      "Epoch 1 | Step 13850/27347 | Avg loss: 0.0327\n",
      "Epoch 1 | Step 13900/27347 | Avg loss: 0.0326\n",
      "Epoch 1 | Step 13950/27347 | Avg loss: 0.0325\n",
      "Epoch 1 | Step 14000/27347 | Avg loss: 0.0325\n",
      "Epoch 1 | Step 14050/27347 | Avg loss: 0.0324\n",
      "Epoch 1 | Step 14100/27347 | Avg loss: 0.0323\n",
      "Epoch 1 | Step 14150/27347 | Avg loss: 0.0322\n",
      "Epoch 1 | Step 14200/27347 | Avg loss: 0.0321\n",
      "Epoch 1 | Step 14250/27347 | Avg loss: 0.0320\n",
      "Epoch 1 | Step 14300/27347 | Avg loss: 0.0319\n",
      "Epoch 1 | Step 14350/27347 | Avg loss: 0.0319\n",
      "Epoch 1 | Step 14400/27347 | Avg loss: 0.0318\n",
      "Epoch 1 | Step 14450/27347 | Avg loss: 0.0317\n",
      "Epoch 1 | Step 14500/27347 | Avg loss: 0.0316\n",
      "Epoch 1 | Step 14550/27347 | Avg loss: 0.0315\n",
      "Epoch 1 | Step 14600/27347 | Avg loss: 0.0314\n",
      "Epoch 1 | Step 14650/27347 | Avg loss: 0.0314\n",
      "Epoch 1 | Step 14700/27347 | Avg loss: 0.0313\n",
      "Epoch 1 | Step 14750/27347 | Avg loss: 0.0312\n",
      "Epoch 1 | Step 14800/27347 | Avg loss: 0.0311\n",
      "Epoch 1 | Step 14850/27347 | Avg loss: 0.0310\n",
      "Epoch 1 | Step 14900/27347 | Avg loss: 0.0310\n",
      "Epoch 1 | Step 14950/27347 | Avg loss: 0.0309\n",
      "Epoch 1 | Step 15000/27347 | Avg loss: 0.0308\n",
      "Epoch 1 | Step 15050/27347 | Avg loss: 0.0307\n",
      "Epoch 1 | Step 15100/27347 | Avg loss: 0.0306\n",
      "Epoch 1 | Step 15150/27347 | Avg loss: 0.0305\n",
      "Epoch 1 | Step 15200/27347 | Avg loss: 0.0304\n",
      "Epoch 1 | Step 15250/27347 | Avg loss: 0.0304\n",
      "Epoch 1 | Step 15300/27347 | Avg loss: 0.0303\n",
      "Epoch 1 | Step 15350/27347 | Avg loss: 0.0302\n",
      "Epoch 1 | Step 15400/27347 | Avg loss: 0.0301\n",
      "Epoch 1 | Step 15450/27347 | Avg loss: 0.0300\n",
      "Epoch 1 | Step 15500/27347 | Avg loss: 0.0299\n",
      "Epoch 1 | Step 15550/27347 | Avg loss: 0.0299\n",
      "Epoch 1 | Step 15600/27347 | Avg loss: 0.0298\n",
      "Epoch 1 | Step 15650/27347 | Avg loss: 0.0297\n",
      "Epoch 1 | Step 15700/27347 | Avg loss: 0.0296\n",
      "Epoch 1 | Step 15750/27347 | Avg loss: 0.0295\n",
      "Epoch 1 | Step 15800/27347 | Avg loss: 0.0294\n",
      "Epoch 1 | Step 15850/27347 | Avg loss: 0.0294\n",
      "Epoch 1 | Step 15900/27347 | Avg loss: 0.0293\n",
      "Epoch 1 | Step 15950/27347 | Avg loss: 0.0292\n",
      "Epoch 1 | Step 16000/27347 | Avg loss: 0.0291\n",
      "Epoch 1 | Step 16050/27347 | Avg loss: 0.0291\n",
      "Epoch 1 | Step 16100/27347 | Avg loss: 0.0290\n",
      "Epoch 1 | Step 16150/27347 | Avg loss: 0.0290\n",
      "Epoch 1 | Step 16200/27347 | Avg loss: 0.0289\n",
      "Epoch 1 | Step 16250/27347 | Avg loss: 0.0288\n",
      "Epoch 1 | Step 16300/27347 | Avg loss: 0.0287\n",
      "Epoch 1 | Step 16350/27347 | Avg loss: 0.0287\n",
      "Epoch 1 | Step 16400/27347 | Avg loss: 0.0286\n",
      "Epoch 1 | Step 16450/27347 | Avg loss: 0.0285\n",
      "Epoch 1 | Step 16500/27347 | Avg loss: 0.0285\n",
      "Epoch 1 | Step 16550/27347 | Avg loss: 0.0284\n",
      "Epoch 1 | Step 16600/27347 | Avg loss: 0.0284\n",
      "Epoch 1 | Step 16650/27347 | Avg loss: 0.0283\n",
      "Epoch 1 | Step 16700/27347 | Avg loss: 0.0282\n",
      "Epoch 1 | Step 16750/27347 | Avg loss: 0.0282\n",
      "Epoch 1 | Step 16800/27347 | Avg loss: 0.0281\n",
      "Epoch 1 | Step 16850/27347 | Avg loss: 0.0280\n",
      "Epoch 1 | Step 16900/27347 | Avg loss: 0.0279\n",
      "Epoch 1 | Step 16950/27347 | Avg loss: 0.0279\n",
      "Epoch 1 | Step 17000/27347 | Avg loss: 0.0279\n",
      "Epoch 1 | Step 17050/27347 | Avg loss: 0.0279\n",
      "Epoch 1 | Step 17100/27347 | Avg loss: 0.0278\n",
      "Epoch 1 | Step 17150/27347 | Avg loss: 0.0277\n",
      "Epoch 1 | Step 17200/27347 | Avg loss: 0.0277\n",
      "Epoch 1 | Step 17250/27347 | Avg loss: 0.0276\n",
      "Epoch 1 | Step 17300/27347 | Avg loss: 0.0275\n",
      "Epoch 1 | Step 17350/27347 | Avg loss: 0.0275\n",
      "Epoch 1 | Step 17400/27347 | Avg loss: 0.0274\n",
      "Epoch 1 | Step 17450/27347 | Avg loss: 0.0273\n",
      "Epoch 1 | Step 17500/27347 | Avg loss: 0.0273\n",
      "Epoch 1 | Step 17550/27347 | Avg loss: 0.0272\n",
      "Epoch 1 | Step 17600/27347 | Avg loss: 0.0271\n",
      "Epoch 1 | Step 17650/27347 | Avg loss: 0.0271\n",
      "Epoch 1 | Step 17700/27347 | Avg loss: 0.0270\n",
      "Epoch 1 | Step 17750/27347 | Avg loss: 0.0270\n",
      "Epoch 1 | Step 17800/27347 | Avg loss: 0.0269\n",
      "Epoch 1 | Step 17850/27347 | Avg loss: 0.0268\n",
      "Epoch 1 | Step 17900/27347 | Avg loss: 0.0268\n",
      "Epoch 1 | Step 17950/27347 | Avg loss: 0.0267\n",
      "Epoch 1 | Step 18000/27347 | Avg loss: 0.0267\n",
      "Epoch 1 | Step 18050/27347 | Avg loss: 0.0266\n",
      "Epoch 1 | Step 18100/27347 | Avg loss: 0.0266\n",
      "Epoch 1 | Step 18150/27347 | Avg loss: 0.0266\n",
      "Epoch 1 | Step 18200/27347 | Avg loss: 0.0265\n",
      "Epoch 1 | Step 18250/27347 | Avg loss: 0.0264\n",
      "Epoch 1 | Step 18300/27347 | Avg loss: 0.0264\n",
      "Epoch 1 | Step 18350/27347 | Avg loss: 0.0263\n",
      "Epoch 1 | Step 18400/27347 | Avg loss: 0.0262\n",
      "Epoch 1 | Step 18450/27347 | Avg loss: 0.0262\n",
      "Epoch 1 | Step 18500/27347 | Avg loss: 0.0261\n",
      "Epoch 1 | Step 18550/27347 | Avg loss: 0.0261\n",
      "Epoch 1 | Step 18600/27347 | Avg loss: 0.0260\n",
      "Epoch 1 | Step 18650/27347 | Avg loss: 0.0260\n",
      "Epoch 1 | Step 18700/27347 | Avg loss: 0.0259\n",
      "Epoch 1 | Step 18750/27347 | Avg loss: 0.0258\n",
      "Epoch 1 | Step 18800/27347 | Avg loss: 0.0258\n",
      "Epoch 1 | Step 18850/27347 | Avg loss: 0.0257\n",
      "Epoch 1 | Step 18900/27347 | Avg loss: 0.0257\n",
      "Epoch 1 | Step 18950/27347 | Avg loss: 0.0256\n",
      "Epoch 1 | Step 19000/27347 | Avg loss: 0.0256\n",
      "Epoch 1 | Step 19050/27347 | Avg loss: 0.0255\n",
      "Epoch 1 | Step 19100/27347 | Avg loss: 0.0255\n",
      "Epoch 1 | Step 19150/27347 | Avg loss: 0.0255\n",
      "Epoch 1 | Step 19200/27347 | Avg loss: 0.0254\n",
      "Epoch 1 | Step 19250/27347 | Avg loss: 0.0253\n",
      "Epoch 1 | Step 19300/27347 | Avg loss: 0.0253\n",
      "Epoch 1 | Step 19350/27347 | Avg loss: 0.0252\n",
      "Epoch 1 | Step 19400/27347 | Avg loss: 0.0252\n",
      "Epoch 1 | Step 19450/27347 | Avg loss: 0.0251\n",
      "Epoch 1 | Step 19500/27347 | Avg loss: 0.0251\n",
      "Epoch 1 | Step 19550/27347 | Avg loss: 0.0250\n",
      "Epoch 1 | Step 19600/27347 | Avg loss: 0.0250\n",
      "Epoch 1 | Step 19650/27347 | Avg loss: 0.0249\n",
      "Epoch 1 | Step 19700/27347 | Avg loss: 0.0248\n",
      "Epoch 1 | Step 19750/27347 | Avg loss: 0.0248\n",
      "Epoch 1 | Step 19800/27347 | Avg loss: 0.0247\n",
      "Epoch 1 | Step 19850/27347 | Avg loss: 0.0247\n",
      "Epoch 1 | Step 19900/27347 | Avg loss: 0.0246\n",
      "Epoch 1 | Step 19950/27347 | Avg loss: 0.0246\n",
      "Epoch 1 | Step 20000/27347 | Avg loss: 0.0246\n",
      "Epoch 1 | Step 20050/27347 | Avg loss: 0.0245\n",
      "Epoch 1 | Step 20100/27347 | Avg loss: 0.0245\n",
      "Epoch 1 | Step 20150/27347 | Avg loss: 0.0244\n",
      "Epoch 1 | Step 20200/27347 | Avg loss: 0.0244\n",
      "Epoch 1 | Step 20250/27347 | Avg loss: 0.0243\n",
      "Epoch 1 | Step 20300/27347 | Avg loss: 0.0243\n",
      "Epoch 1 | Step 20350/27347 | Avg loss: 0.0242\n",
      "Epoch 1 | Step 20400/27347 | Avg loss: 0.0242\n",
      "Epoch 1 | Step 20450/27347 | Avg loss: 0.0241\n",
      "Epoch 1 | Step 20500/27347 | Avg loss: 0.0241\n",
      "Epoch 1 | Step 20550/27347 | Avg loss: 0.0240\n",
      "Epoch 1 | Step 20600/27347 | Avg loss: 0.0240\n",
      "Epoch 1 | Step 20650/27347 | Avg loss: 0.0239\n",
      "Epoch 1 | Step 20700/27347 | Avg loss: 0.0239\n",
      "Epoch 1 | Step 20750/27347 | Avg loss: 0.0238\n",
      "Epoch 1 | Step 20800/27347 | Avg loss: 0.0238\n",
      "Epoch 1 | Step 20850/27347 | Avg loss: 0.0237\n",
      "Epoch 1 | Step 20900/27347 | Avg loss: 0.0237\n",
      "Epoch 1 | Step 20950/27347 | Avg loss: 0.0236\n",
      "Epoch 1 | Step 21000/27347 | Avg loss: 0.0236\n",
      "Epoch 1 | Step 21050/27347 | Avg loss: 0.0235\n",
      "Epoch 1 | Step 21100/27347 | Avg loss: 0.0235\n",
      "Epoch 1 | Step 21150/27347 | Avg loss: 0.0234\n",
      "Epoch 1 | Step 21200/27347 | Avg loss: 0.0234\n",
      "Epoch 1 | Step 21250/27347 | Avg loss: 0.0233\n",
      "Epoch 1 | Step 21300/27347 | Avg loss: 0.0233\n",
      "Epoch 1 | Step 21350/27347 | Avg loss: 0.0232\n",
      "Epoch 1 | Step 21400/27347 | Avg loss: 0.0232\n",
      "Epoch 1 | Step 21450/27347 | Avg loss: 0.0231\n",
      "Epoch 1 | Step 21500/27347 | Avg loss: 0.0231\n",
      "Epoch 1 | Step 21550/27347 | Avg loss: 0.0230\n",
      "Epoch 1 | Step 21600/27347 | Avg loss: 0.0230\n",
      "Epoch 1 | Step 21650/27347 | Avg loss: 0.0229\n",
      "Epoch 1 | Step 21700/27347 | Avg loss: 0.0229\n",
      "Epoch 1 | Step 21750/27347 | Avg loss: 0.0228\n",
      "Epoch 1 | Step 21800/27347 | Avg loss: 0.0228\n",
      "Epoch 1 | Step 21850/27347 | Avg loss: 0.0228\n",
      "Epoch 1 | Step 21900/27347 | Avg loss: 0.0227\n",
      "Epoch 1 | Step 21950/27347 | Avg loss: 0.0227\n",
      "Epoch 1 | Step 22000/27347 | Avg loss: 0.0227\n",
      "Epoch 1 | Step 22050/27347 | Avg loss: 0.0226\n",
      "Epoch 1 | Step 22100/27347 | Avg loss: 0.0226\n",
      "Epoch 1 | Step 22150/27347 | Avg loss: 0.0225\n",
      "Epoch 1 | Step 22200/27347 | Avg loss: 0.0225\n",
      "Epoch 1 | Step 22250/27347 | Avg loss: 0.0225\n",
      "Epoch 1 | Step 22300/27347 | Avg loss: 0.0224\n",
      "Epoch 1 | Step 22350/27347 | Avg loss: 0.0224\n",
      "Epoch 1 | Step 22400/27347 | Avg loss: 0.0223\n",
      "Epoch 1 | Step 22450/27347 | Avg loss: 0.0223\n",
      "Epoch 1 | Step 22500/27347 | Avg loss: 0.0223\n",
      "Epoch 1 | Step 22550/27347 | Avg loss: 0.0222\n",
      "Epoch 1 | Step 22600/27347 | Avg loss: 0.0222\n",
      "Epoch 1 | Step 22650/27347 | Avg loss: 0.0221\n",
      "Epoch 1 | Step 22700/27347 | Avg loss: 0.0221\n",
      "Epoch 1 | Step 22750/27347 | Avg loss: 0.0220\n",
      "Epoch 1 | Step 22800/27347 | Avg loss: 0.0220\n",
      "Epoch 1 | Step 22850/27347 | Avg loss: 0.0220\n",
      "Epoch 1 | Step 22900/27347 | Avg loss: 0.0219\n",
      "Epoch 1 | Step 22950/27347 | Avg loss: 0.0219\n",
      "Epoch 1 | Step 23000/27347 | Avg loss: 0.0218\n",
      "Epoch 1 | Step 23050/27347 | Avg loss: 0.0218\n",
      "Epoch 1 | Step 23100/27347 | Avg loss: 0.0217\n",
      "Epoch 1 | Step 23150/27347 | Avg loss: 0.0217\n",
      "Epoch 1 | Step 23200/27347 | Avg loss: 0.0217\n",
      "Epoch 1 | Step 23250/27347 | Avg loss: 0.0216\n",
      "Epoch 1 | Step 23300/27347 | Avg loss: 0.0216\n",
      "Epoch 1 | Step 23350/27347 | Avg loss: 0.0215\n",
      "Epoch 1 | Step 23400/27347 | Avg loss: 0.0215\n",
      "Epoch 1 | Step 23450/27347 | Avg loss: 0.0215\n",
      "Epoch 1 | Step 23500/27347 | Avg loss: 0.0215\n",
      "Epoch 1 | Step 23550/27347 | Avg loss: 0.0214\n",
      "Epoch 1 | Step 23600/27347 | Avg loss: 0.0214\n",
      "Epoch 1 | Step 23650/27347 | Avg loss: 0.0214\n",
      "Epoch 1 | Step 23700/27347 | Avg loss: 0.0213\n",
      "Epoch 1 | Step 23750/27347 | Avg loss: 0.0213\n",
      "Epoch 1 | Step 23800/27347 | Avg loss: 0.0212\n",
      "Epoch 1 | Step 23850/27347 | Avg loss: 0.0212\n",
      "Epoch 1 | Step 23900/27347 | Avg loss: 0.0212\n",
      "Epoch 1 | Step 23950/27347 | Avg loss: 0.0211\n",
      "Epoch 1 | Step 24000/27347 | Avg loss: 0.0211\n",
      "Epoch 1 | Step 24050/27347 | Avg loss: 0.0211\n",
      "Epoch 1 | Step 24100/27347 | Avg loss: 0.0210\n",
      "Epoch 1 | Step 24150/27347 | Avg loss: 0.0210\n",
      "Epoch 1 | Step 24200/27347 | Avg loss: 0.0209\n",
      "Epoch 1 | Step 24250/27347 | Avg loss: 0.0209\n",
      "Epoch 1 | Step 24300/27347 | Avg loss: 0.0209\n",
      "Epoch 1 | Step 24350/27347 | Avg loss: 0.0208\n",
      "Epoch 1 | Step 24400/27347 | Avg loss: 0.0208\n",
      "Epoch 1 | Step 24450/27347 | Avg loss: 0.0208\n",
      "Epoch 1 | Step 24500/27347 | Avg loss: 0.0207\n",
      "Epoch 1 | Step 24550/27347 | Avg loss: 0.0207\n",
      "Epoch 1 | Step 24600/27347 | Avg loss: 0.0207\n",
      "Epoch 1 | Step 24650/27347 | Avg loss: 0.0206\n",
      "Epoch 1 | Step 24700/27347 | Avg loss: 0.0206\n",
      "Epoch 1 | Step 24750/27347 | Avg loss: 0.0206\n",
      "Epoch 1 | Step 24800/27347 | Avg loss: 0.0205\n",
      "Epoch 1 | Step 24850/27347 | Avg loss: 0.0205\n",
      "Epoch 1 | Step 24900/27347 | Avg loss: 0.0205\n",
      "Epoch 1 | Step 24950/27347 | Avg loss: 0.0204\n",
      "Epoch 1 | Step 25000/27347 | Avg loss: 0.0204\n",
      "Epoch 1 | Step 25050/27347 | Avg loss: 0.0204\n",
      "Epoch 1 | Step 25100/27347 | Avg loss: 0.0203\n",
      "Epoch 1 | Step 25150/27347 | Avg loss: 0.0203\n",
      "Epoch 1 | Step 25200/27347 | Avg loss: 0.0203\n",
      "Epoch 1 | Step 25250/27347 | Avg loss: 0.0202\n",
      "Epoch 1 | Step 25300/27347 | Avg loss: 0.0202\n",
      "Epoch 1 | Step 25350/27347 | Avg loss: 0.0202\n",
      "Epoch 1 | Step 25400/27347 | Avg loss: 0.0201\n",
      "Epoch 1 | Step 25450/27347 | Avg loss: 0.0201\n",
      "Epoch 1 | Step 25500/27347 | Avg loss: 0.0201\n",
      "Epoch 1 | Step 25550/27347 | Avg loss: 0.0201\n",
      "Epoch 1 | Step 25600/27347 | Avg loss: 0.0200\n",
      "Epoch 1 | Step 25650/27347 | Avg loss: 0.0200\n",
      "Epoch 1 | Step 25700/27347 | Avg loss: 0.0200\n",
      "Epoch 1 | Step 25750/27347 | Avg loss: 0.0200\n",
      "Epoch 1 | Step 25800/27347 | Avg loss: 0.0199\n",
      "Epoch 1 | Step 25850/27347 | Avg loss: 0.0199\n",
      "Epoch 1 | Step 25900/27347 | Avg loss: 0.0198\n",
      "Epoch 1 | Step 25950/27347 | Avg loss: 0.0198\n",
      "Epoch 1 | Step 26000/27347 | Avg loss: 0.0198\n",
      "Epoch 1 | Step 26050/27347 | Avg loss: 0.0198\n",
      "Epoch 1 | Step 26100/27347 | Avg loss: 0.0197\n",
      "Epoch 1 | Step 26150/27347 | Avg loss: 0.0197\n",
      "Epoch 1 | Step 26200/27347 | Avg loss: 0.0197\n",
      "Epoch 1 | Step 26250/27347 | Avg loss: 0.0196\n",
      "Epoch 1 | Step 26300/27347 | Avg loss: 0.0196\n",
      "Epoch 1 | Step 26350/27347 | Avg loss: 0.0196\n",
      "Epoch 1 | Step 26400/27347 | Avg loss: 0.0196\n",
      "Epoch 1 | Step 26450/27347 | Avg loss: 0.0195\n",
      "Epoch 1 | Step 26500/27347 | Avg loss: 0.0195\n",
      "Epoch 1 | Step 26550/27347 | Avg loss: 0.0195\n",
      "Epoch 1 | Step 26600/27347 | Avg loss: 0.0195\n",
      "Epoch 1 | Step 26650/27347 | Avg loss: 0.0194\n",
      "Epoch 1 | Step 26700/27347 | Avg loss: 0.0194\n",
      "Epoch 1 | Step 26750/27347 | Avg loss: 0.0194\n",
      "Epoch 1 | Step 26800/27347 | Avg loss: 0.0193\n",
      "Epoch 1 | Step 26850/27347 | Avg loss: 0.0193\n",
      "Epoch 1 | Step 26900/27347 | Avg loss: 0.0193\n",
      "Epoch 1 | Step 26950/27347 | Avg loss: 0.0193\n",
      "Epoch 1 | Step 27000/27347 | Avg loss: 0.0192\n",
      "Epoch 1 | Step 27050/27347 | Avg loss: 0.0192\n",
      "Epoch 1 | Step 27100/27347 | Avg loss: 0.0192\n",
      "Epoch 1 | Step 27150/27347 | Avg loss: 0.0191\n",
      "Epoch 1 | Step 27200/27347 | Avg loss: 0.0191\n",
      "Epoch 1 | Step 27250/27347 | Avg loss: 0.0191\n",
      "Epoch 1 | Step 27300/27347 | Avg loss: 0.0190\n",
      "Epoch 1 finished. Average training loss: 0.0190\n",
      "Epoch 2 | Step 50/27347 | Avg loss: 0.0139\n",
      "Epoch 2 | Step 100/27347 | Avg loss: 0.0094\n",
      "Epoch 2 | Step 150/27347 | Avg loss: 0.0068\n",
      "Epoch 2 | Step 200/27347 | Avg loss: 0.0053\n",
      "Epoch 2 | Step 250/27347 | Avg loss: 0.0044\n",
      "Epoch 2 | Step 300/27347 | Avg loss: 0.0043\n",
      "Epoch 2 | Step 350/27347 | Avg loss: 0.0037\n",
      "Epoch 2 | Step 400/27347 | Avg loss: 0.0033\n",
      "Epoch 2 | Step 450/27347 | Avg loss: 0.0032\n",
      "Epoch 2 | Step 500/27347 | Avg loss: 0.0029\n",
      "Epoch 2 | Step 550/27347 | Avg loss: 0.0029\n",
      "Epoch 2 | Step 600/27347 | Avg loss: 0.0028\n",
      "Epoch 2 | Step 650/27347 | Avg loss: 0.0027\n",
      "Epoch 2 | Step 700/27347 | Avg loss: 0.0025\n",
      "Epoch 2 | Step 750/27347 | Avg loss: 0.0025\n",
      "Epoch 2 | Step 800/27347 | Avg loss: 0.0024\n",
      "Epoch 2 | Step 850/27347 | Avg loss: 0.0023\n",
      "Epoch 2 | Step 900/27347 | Avg loss: 0.0023\n",
      "Epoch 2 | Step 950/27347 | Avg loss: 0.0025\n",
      "Epoch 2 | Step 1000/27347 | Avg loss: 0.0025\n",
      "Epoch 2 | Step 1050/27347 | Avg loss: 0.0025\n",
      "Epoch 2 | Step 1100/27347 | Avg loss: 0.0025\n",
      "Epoch 2 | Step 1150/27347 | Avg loss: 0.0025\n",
      "Epoch 2 | Step 1200/27347 | Avg loss: 0.0024\n",
      "Epoch 2 | Step 1250/27347 | Avg loss: 0.0024\n",
      "Epoch 2 | Step 1300/27347 | Avg loss: 0.0023\n",
      "Epoch 2 | Step 1350/27347 | Avg loss: 0.0023\n",
      "Epoch 2 | Step 1400/27347 | Avg loss: 0.0022\n",
      "Epoch 2 | Step 1450/27347 | Avg loss: 0.0022\n",
      "Epoch 2 | Step 1500/27347 | Avg loss: 0.0023\n",
      "Epoch 2 | Step 1550/27347 | Avg loss: 0.0023\n",
      "Epoch 2 | Step 1600/27347 | Avg loss: 0.0022\n",
      "Epoch 2 | Step 1650/27347 | Avg loss: 0.0022\n",
      "Epoch 2 | Step 1700/27347 | Avg loss: 0.0022\n",
      "Epoch 2 | Step 1750/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 1800/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 1850/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 1900/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 1950/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 2000/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 2050/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 2100/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 2150/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 2200/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 2250/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 2300/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 2350/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 2400/27347 | Avg loss: 0.0023\n",
      "Epoch 2 | Step 2450/27347 | Avg loss: 0.0022\n",
      "Epoch 2 | Step 2500/27347 | Avg loss: 0.0022\n",
      "Epoch 2 | Step 2550/27347 | Avg loss: 0.0022\n",
      "Epoch 2 | Step 2600/27347 | Avg loss: 0.0022\n",
      "Epoch 2 | Step 2650/27347 | Avg loss: 0.0022\n",
      "Epoch 2 | Step 2700/27347 | Avg loss: 0.0022\n",
      "Epoch 2 | Step 2750/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 2800/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 2850/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 2900/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 2950/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 3000/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 3050/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 3100/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 3150/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 3200/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 3250/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 3300/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 3350/27347 | Avg loss: 0.0022\n",
      "Epoch 2 | Step 3400/27347 | Avg loss: 0.0022\n",
      "Epoch 2 | Step 3450/27347 | Avg loss: 0.0022\n",
      "Epoch 2 | Step 3500/27347 | Avg loss: 0.0022\n",
      "Epoch 2 | Step 3550/27347 | Avg loss: 0.0022\n",
      "Epoch 2 | Step 3600/27347 | Avg loss: 0.0022\n",
      "Epoch 2 | Step 3650/27347 | Avg loss: 0.0022\n",
      "Epoch 2 | Step 3700/27347 | Avg loss: 0.0022\n",
      "Epoch 2 | Step 3750/27347 | Avg loss: 0.0022\n",
      "Epoch 2 | Step 3800/27347 | Avg loss: 0.0022\n",
      "Epoch 2 | Step 3850/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 3900/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 3950/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 4000/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 4050/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 4100/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 4150/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 4200/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 4250/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 4300/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 4350/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 4400/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 4450/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 4500/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 4550/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 4600/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 4650/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 4700/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 4750/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 4800/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 4850/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 4900/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 4950/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 5000/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 5050/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 5100/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 5150/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 5200/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 5250/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 5300/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 5350/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 5400/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 5450/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 5500/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 5550/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 5600/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 5650/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 5700/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 5750/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 5800/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 5850/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 5900/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 5950/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 6000/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 6050/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 6100/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 6150/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 6200/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 6250/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 6300/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 6350/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 6400/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 6450/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 6500/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 6550/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 6600/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 6650/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 6700/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 6750/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 6800/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 6850/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 6900/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 6950/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 7000/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 7050/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 7100/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 7150/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 7200/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 7250/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 7300/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 7350/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 7400/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 7450/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 7500/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 7550/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 7600/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 7650/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 7700/27347 | Avg loss: 0.0022\n",
      "Epoch 2 | Step 7750/27347 | Avg loss: 0.0022\n",
      "Epoch 2 | Step 7800/27347 | Avg loss: 0.0022\n",
      "Epoch 2 | Step 7850/27347 | Avg loss: 0.0022\n",
      "Epoch 2 | Step 7900/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 7950/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 8000/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 8050/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 8100/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 8150/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 8200/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 8250/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 8300/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 8350/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 8400/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 8450/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 8500/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 8550/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 8600/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 8650/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 8700/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 8750/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 8800/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 8850/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 8900/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 8950/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 9000/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 9050/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 9100/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 9150/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 9200/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 9250/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 9300/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 9350/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 9400/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 9450/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 9500/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 9550/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 9600/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 9650/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 9700/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 9750/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 9800/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 9850/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 9900/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 9950/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 10000/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 10050/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 10100/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 10150/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 10200/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 10250/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 10300/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 10350/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 10400/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 10450/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 10500/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 10550/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 10600/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 10650/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 10700/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 10750/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 10800/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 10850/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 10900/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 10950/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 11000/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 11050/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 11100/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 11150/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 11200/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 11250/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 11300/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 11350/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 11400/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 11450/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 11500/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 11550/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 11600/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 11650/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 11700/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 11750/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 11800/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 11850/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 11900/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 11950/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 12000/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 12050/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 12100/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 12150/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 12200/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 12250/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 12300/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 12350/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 12400/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 12450/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 12500/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 12550/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 12600/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 12650/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 12700/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 12750/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 12800/27347 | Avg loss: 0.0021\n",
      "Epoch 2 | Step 12850/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 12900/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 12950/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 13000/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 13050/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 13100/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 13150/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 13200/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 13250/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 13300/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 13350/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 13400/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 13450/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 13500/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 13550/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 13600/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 13650/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 13700/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 13750/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 13800/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 13850/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 13900/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 13950/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 14000/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 14050/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 14100/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 14150/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 14200/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 14250/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 14300/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 14350/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 14400/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 14450/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 14500/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 14550/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 14600/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 14650/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 14700/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 14750/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 14800/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 14850/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 14900/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 14950/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 15000/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 15050/27347 | Avg loss: 0.0020\n",
      "Epoch 2 | Step 15100/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 15150/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 15200/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 15250/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 15300/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 15350/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 15400/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 15450/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 15500/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 15550/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 15600/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 15650/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 15700/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 15750/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 15800/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 15850/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 15900/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 15950/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 16000/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 16050/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 16100/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 16150/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 16200/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 16250/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 16300/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 16350/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 16400/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 16450/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 16500/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 16550/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 16600/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 16650/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 16700/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 16750/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 16800/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 16850/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 16900/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 16950/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 17000/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 17050/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 17100/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 17150/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 17200/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 17250/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 17300/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 17350/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 17400/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 17450/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 17500/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 17550/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 17600/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 17650/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 17700/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 17750/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 17800/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 17850/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 17900/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 17950/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 18000/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 18050/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 18100/27347 | Avg loss: 0.0019\n",
      "Epoch 2 | Step 18150/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 18200/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 18250/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 18300/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 18350/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 18400/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 18450/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 18500/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 18550/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 18600/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 18650/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 18700/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 18750/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 18800/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 18850/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 18900/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 18950/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 19000/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 19050/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 19100/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 19150/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 19200/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 19250/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 19300/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 19350/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 19400/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 19450/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 19500/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 19550/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 19600/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 19650/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 19700/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 19750/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 19800/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 19850/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 19900/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 19950/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 20000/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 20050/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 20100/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 20150/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 20200/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 20250/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 20300/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 20350/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 20400/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 20450/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 20500/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 20550/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 20600/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 20650/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 20700/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 20750/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 20800/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 20850/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 20900/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 20950/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 21000/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 21050/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 21100/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 21150/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 21200/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 21250/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 21300/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 21350/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 21400/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 21450/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 21500/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 21550/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 21600/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 21650/27347 | Avg loss: 0.0018\n",
      "Epoch 2 | Step 21700/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 21750/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 21800/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 21850/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 21900/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 21950/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 22000/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 22050/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 22100/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 22150/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 22200/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 22250/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 22300/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 22350/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 22400/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 22450/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 22500/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 22550/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 22600/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 22650/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 22700/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 22750/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 22800/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 22850/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 22900/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 22950/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 23000/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 23050/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 23100/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 23150/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 23200/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 23250/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 23300/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 23350/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 23400/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 23450/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 23500/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 23550/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 23600/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 23650/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 23700/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 23750/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 23800/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 23850/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 23900/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 23950/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 24000/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 24050/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 24100/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 24150/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 24200/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 24250/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 24300/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 24350/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 24400/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 24450/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 24500/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 24550/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 24600/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 24650/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 24700/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 24750/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 24800/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 24850/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 24900/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 24950/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 25000/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 25050/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 25100/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 25150/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 25200/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 25250/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 25300/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 25350/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 25400/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 25450/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 25500/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 25550/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 25600/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 25650/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 25700/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 25750/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 25800/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 25850/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 25900/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 25950/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 26000/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 26050/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 26100/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 26150/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 26200/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 26250/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 26300/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 26350/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 26400/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 26450/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 26500/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 26550/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 26600/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 26650/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 26700/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 26750/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 26800/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 26850/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 26900/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 26950/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 27000/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 27050/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 27100/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 27150/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 27200/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 27250/27347 | Avg loss: 0.0017\n",
      "Epoch 2 | Step 27300/27347 | Avg loss: 0.0017\n",
      "Epoch 2 finished. Average training loss: 0.0017\n"
     ]
    }
   ],
   "source": [
    "# 5.1 Training loop\n",
    "# this part took me 4 hours to run so I already saved this part so we can just import it later easy peasy\n",
    "\n",
    "dual_encoder.train()\n",
    "set_seed(42)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    dual_encoder.train()\n",
    "\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        q_input_ids = batch[\"q_input_ids\"].to(device)\n",
    "        q_attention_mask = batch[\"q_attention_mask\"].to(device)\n",
    "        c_input_ids = batch[\"c_input_ids\"].to(device)\n",
    "        c_attention_mask = batch[\"c_attention_mask\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        q_emb, c_emb = dual_encoder(\n",
    "            q_input_ids=q_input_ids,\n",
    "            q_attention_mask=q_attention_mask,\n",
    "            c_input_ids=c_input_ids,\n",
    "            c_attention_mask=c_attention_mask\n",
    "        )\n",
    "\n",
    "        loss = contrastive_loss(q_emb, c_emb, temperature=0.05)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if (step + 1) % 50 == 0:\n",
    "            avg_loss = total_loss / (step + 1)\n",
    "            print(f\"Epoch {epoch+1} | Step {step+1}/{len(train_loader)} | Avg loss: {avg_loss:.4f}\")\n",
    "\n",
    "    avg_epoch_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} finished. Average training loss: {avg_epoch_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0214280-11a1-40ea-be06-ca103a4f31d5",
   "metadata": {},
   "source": [
    "# 6. Build context embedding index for retrieval\n",
    "Now we encode all contexts with the trained dual encoder to build a retrieval index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69fc9476-46dc-45bb-9d3d-966cc0a72c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Build context embedding index for all contexts\n",
    "\n",
    "class ContextDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.contexts = df[\"context\"].tolist()\n",
    "        self.ids = df[\"id\"].tolist()\n",
    "    def __len__(self):\n",
    "        return len(self.contexts)\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"context\": self.contexts[idx],\n",
    "            \"id\": self.ids[idx]\n",
    "        }\n",
    "\n",
    "context_dataset = ContextDataset(df_all)\n",
    "\n",
    "def context_collate_fn(batch):\n",
    "    cs = [b[\"context\"] for b in batch]\n",
    "    ids = [b[\"id\"] for b in batch]\n",
    "    enc = tokenizer(\n",
    "        cs,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return {\n",
    "        \"c_input_ids\": enc[\"input_ids\"],\n",
    "        \"c_attention_mask\": enc[\"attention_mask\"],\n",
    "        \"ids\": torch.tensor(ids, dtype=torch.long)\n",
    "    }\n",
    "\n",
    "context_loader = DataLoader(\n",
    "    context_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=context_collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a31f5ec6-fc89-4043-896b-5ec4c3c42980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(273467, 768)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6.1 Encode all contexts\n",
    "\n",
    "dual_encoder.eval()\n",
    "\n",
    "hidden_size = dual_encoder.hidden_size\n",
    "num_contexts = len(df_all)\n",
    "context_embs = np.zeros((num_contexts, hidden_size), dtype=np.float32)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in context_loader:\n",
    "        c_input_ids = batch[\"c_input_ids\"].to(device)\n",
    "        c_attention_mask = batch[\"c_attention_mask\"].to(device)\n",
    "        ids = batch[\"ids\"].numpy() # global ids\n",
    "\n",
    "        c_emb = dual_encoder.encode(c_input_ids, c_attention_mask)  # [B, H]\n",
    "        c_emb = c_emb.cpu().numpy()\n",
    "\n",
    "        context_embs[ids] = c_emb\n",
    "\n",
    "context_embs.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ebce40-41d9-486d-b4e4-37f1e16afb5b",
   "metadata": {},
   "source": [
    "# 7. Self-retrieval evaluation on the test set\n",
    "We check: for each test question, does the model retrieve its own context in top-k?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d90ff46-4653-411e-8c78-51ed15edb33f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27347"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7. Self-retrieval evaluation: Recall@k and MRR\n",
    "\n",
    "dual_encoder.eval()\n",
    "\n",
    "test_questions = test_df[\"question\"].tolist()\n",
    "test_true_ids = test_df[\"id\"].tolist()\n",
    "\n",
    "ranks = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for q_text, true_id in zip(test_questions, test_true_ids):\n",
    "        enc = tokenizer(\n",
    "            q_text,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        q_input_ids = enc[\"input_ids\"].to(device)\n",
    "        q_attention_mask = enc[\"attention_mask\"].to(device)\n",
    "\n",
    "        q_emb = dual_encoder.encode(q_input_ids, q_attention_mask) # [1, H]\n",
    "        q_emb = q_emb.cpu().numpy()[0]\n",
    "\n",
    "        sims = context_embs @ q_emb  # [num_contexts]\n",
    "        ranked_indices = np.argsort(-sims) # descending\n",
    "\n",
    "        # rank of the true context (1-based)\n",
    "        rank = np.where(ranked_indices == true_id)[0][0] + 1\n",
    "        ranks.append(rank)\n",
    "\n",
    "ranks = np.array(ranks)\n",
    "len(ranks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3359a3be-6c7b-461b-8b42-a030d201980a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@1:  0.7945\n",
      "Recall@5:  0.9273\n",
      "Recall@10: 0.9531\n",
      "MRR:       0.8545\n"
     ]
    }
   ],
   "source": [
    "# 7.1 Compute Recall@k and MRR\n",
    "\n",
    "def recall_at_k(ranks, k):\n",
    "    return np.mean(ranks <= k)\n",
    "\n",
    "recall_1 = recall_at_k(ranks, 1)\n",
    "recall_5 = recall_at_k(ranks, 5)\n",
    "recall_10 = recall_at_k(ranks, 10)\n",
    "\n",
    "mrr = np.mean(1.0 / ranks)\n",
    "\n",
    "print(f\"Recall@1: {recall_1:.4f}\")\n",
    "print(f\"Recall@5: {recall_5:.4f}\")\n",
    "print(f\"Recall@10: {recall_10:.4f}\")\n",
    "print(f\"MRR: {mrr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2ae339-2e7c-4b04-85d9-2cabf2aeb579",
   "metadata": {},
   "source": [
    "# 8. Qualitative example\n",
    "print a few example retrievals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad2c6940-fd3d-40c6-b549-ced27b2db328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "Does molecular Screen identify Cardiac Myosin-Binding Protein-C as a Protein Kinase G-Iα Substrate?\n",
      "\n",
      "True context:\n",
      "Pharmacological activation of cGMP-dependent protein kinase G I (PKGI) has emerged as a therapeutic strategy for humans with heart failure. However, PKG-activating drugs have been limited by hypotension arising from PKG-induced vasodilation. PKGIα antiremodeling substrates specific to the myocardium might provide targets to circumvent this limitation, but currently remain poorly understood. We performed a screen for myocardial proteins interacting with the PKGIα leucine zipper (LZ)-binding domain to identify myocardial-specific PKGI antiremodeling substrates. Our screen identified cardiac myosin-binding protein-C (cMyBP-C), a cardiac myocyte-specific protein, which has been demonstrated to inhibit cardiac remodeling in the phosphorylated state, and when mutated leads to hypertrophic cardio ...\n",
      "\n",
      "Top-3 retrieved contexts:\n",
      "\n",
      "Rank 1 | id=198815 | sim=0.8612\n",
      "Cardiac contractility is regulated by dynamic phosphorylation of sarcomeric proteins by kinases such as cAMP-activated protein kinase A (PKA). Efficient phosphorylation requires that PKA be anchored close to its targets by A-kinase anchoring proteins (AKAPs). Cardiac Myosin Binding Protein-C (cMyBPC) and cardiac troponin I (cTNI) are hypertrophic cardiomyopathy (HCM)-causing sarcomeric proteins which regulate contractility in response to PKA phosphorylation. During a yeast 2-hybrid (Y2H) library screen using a trisphosphorylation mimic of the C1-C2 region of cMyBPC, we identified isoform 4 of myomegalin (MMGL) as an interactor of this N-terminal cMyBPC region. As MMGL has previously been shown to interact with phosphodiesterase 4D, we speculated that it may be a PKA-anchoring protein (AKAP ...\n",
      "\n",
      "Rank 2 | id=224497 | sim=0.7168\n",
      "Smooth muscle myosin monomers self-assemble in solution to form filaments. Phosphorylation of the 20-kD regulatory myosin light chain (MLC20) enhances filament formation. It is not known whether the phosphorylated and non-phosphorylated filaments possess the same structural integrity. We purified myosin from bovine trachealis to form filaments, in ATP-containing zero-calcium solution during a slow dialysis that gradually reduced the ionic strength. Sufficient myosin light chain kinase and phosphatase, as well as calmodulin, were retained after the myosin purification and this enabled phosphorylation of MLC20 within 20-40s after addition of calcium to the filament suspension. The phosphorylated and non-phosphorylated filaments were then partially disassembled by ultrasonification. The exten ...\n",
      "\n",
      "Rank 3 | id=254592 | sim=0.7048\n",
      "Pharmacological activation of cGMP-dependent protein kinase G I (PKGI) has emerged as a therapeutic strategy for humans with heart failure. However, PKG-activating drugs have been limited by hypotension arising from PKG-induced vasodilation. PKGIα antiremodeling substrates specific to the myocardium might provide targets to circumvent this limitation, but currently remain poorly understood. We performed a screen for myocardial proteins interacting with the PKGIα leucine zipper (LZ)-binding domain to identify myocardial-specific PKGI antiremodeling substrates. Our screen identified cardiac myosin-binding protein-C (cMyBP-C), a cardiac myocyte-specific protein, which has been demonstrated to inhibit cardiac remodeling in the phosphorylated state, and when mutated leads to hypertrophic cardio ...\n"
     ]
    }
   ],
   "source": [
    "# 8. Qualitative example: show top-3 retrieved contexts for one test question\n",
    "\n",
    "idx = 0  # change this to look at different examples\n",
    "q_text = test_questions[idx]\n",
    "true_id = test_true_ids[idx]\n",
    "\n",
    "print(\"Question:\")\n",
    "print(q_text)\n",
    "print(\"\\nTrue context:\")\n",
    "print(df_all.loc[df_all[\"id\"] == true_id, \"context\"].values[0][:800], \"...\\n\")\n",
    "\n",
    "enc = tokenizer(\n",
    "    q_text,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=max_length,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    q_emb = dual_encoder.encode(\n",
    "        enc[\"input_ids\"].to(device),\n",
    "        enc[\"attention_mask\"].to(device)\n",
    "    ).cpu().numpy()[0]\n",
    "\n",
    "sims = context_embs @ q_emb\n",
    "ranked_indices = np.argsort(-sims)\n",
    "\n",
    "print(\"Top-3 retrieved contexts:\")\n",
    "for rank_pos in range(3):\n",
    "    cid = ranked_indices[rank_pos]\n",
    "    score = sims[cid]\n",
    "    print(f\"\\nRank {rank_pos+1} | id={cid} | sim={score:.4f}\")\n",
    "    print(df_all.loc[df_all[\"id\"] == cid, \"context\"].values[0][:800], \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb4a5c0-ea2b-4821-94a8-2487aa5e9254",
   "metadata": {},
   "source": [
    "# EXAMPLE 1. Simple QA function: retrieve + show evidence\n",
    "\n",
    "This version:\n",
    "\n",
    "    * Takes a question string\n",
    "    * Finds the most similar abstract in your corpus\n",
    "    * Shows a short “answer” snippet from that abstract + the full context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a809d27-373d-4da8-a8f7-ff660fdbe377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_retrieve(question, top_k_docs=3, snippet_chars=400):\n",
    "    dual_encoder.eval()\n",
    "\n",
    "    # encode the question\n",
    "    enc = tokenizer(\n",
    "        question,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        q_emb = dual_encoder.encode(\n",
    "            enc[\"input_ids\"].to(device),\n",
    "            enc[\"attention_mask\"].to(device)\n",
    "        ).cpu().numpy()[0]  # [hidden_size]\n",
    "\n",
    "    # similarity with all contexts\n",
    "    sims = context_embs @ q_emb  # [num_contexts]\n",
    "    ranked_indices = np.argsort(-sims)  # descending\n",
    "\n",
    "    print(\"QUESTION:\")\n",
    "    print(question)\n",
    "    print(\"\\nTop retrieved documents:\\n\")\n",
    "\n",
    "    for rank_pos in range(top_k_docs):\n",
    "        cid = ranked_indices[rank_pos]\n",
    "        score = sims[cid]\n",
    "        ctx = df_all.loc[df_all[\"id\"] == cid, \"context\"].values[0]\n",
    "\n",
    "        # simple sentence split\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', ctx)\n",
    "        sentences = [s.strip() for s in sentences if len(s.strip()) > 0]\n",
    "\n",
    "        print(f\"=== Document rank {rank_pos+1} | id={cid} | similarity={score:.3f} ===\")\n",
    "        print(\"Answer-like snippet:\")\n",
    "        # show first 2 sentences as \"answer-ish\" snippet\n",
    "        print(\" \".join(sentences[:2])[:snippet_chars], \"...\")\n",
    "        print(\"\\nContext (truncated):\")\n",
    "        print(ctx[:snippet_chars], \"...\")\n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4930c3-f6a4-4e34-ae52-2e43bfd8c7b8",
   "metadata": {},
   "source": [
    "# 2. Demo cell using a real question from your dataset\n",
    "\n",
    "You can demo with a question from test_df (or any split), or type your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0631722-a9f2-4263-8758-e55b8a735312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION:\n",
      "Does molecular Screen identify Cardiac Myosin-Binding Protein-C as a Protein Kinase G-Iα Substrate?\n",
      "\n",
      "Top retrieved documents:\n",
      "\n",
      "=== Document rank 1 | id=198815 | similarity=0.861 ===\n",
      "Answer-like snippet:\n",
      "Cardiac contractility is regulated by dynamic phosphorylation of sarcomeric proteins by kinases such as cAMP-activated protein kinase A (PKA). Efficient phosphorylation requires that PKA be anchored close to its targets by A-kinase anchoring proteins (AKAPs). ...\n",
      "\n",
      "Context (truncated):\n",
      "Cardiac contractility is regulated by dynamic phosphorylation of sarcomeric proteins by kinases such as cAMP-activated protein kinase A (PKA). Efficient phosphorylation requires that PKA be anchored close to its targets by A-kinase anchoring proteins (AKAPs). Cardiac Myosin Binding Protein-C (cMyBPC) and cardiac troponin I (cTNI) are hypertrophic cardiomyopathy (HCM)-causing sarcomeric proteins wh ...\n",
      "\n",
      "\n",
      "=== Document rank 2 | id=224497 | similarity=0.717 ===\n",
      "Answer-like snippet:\n",
      "Smooth muscle myosin monomers self-assemble in solution to form filaments. Phosphorylation of the 20-kD regulatory myosin light chain (MLC20) enhances filament formation. ...\n",
      "\n",
      "Context (truncated):\n",
      "Smooth muscle myosin monomers self-assemble in solution to form filaments. Phosphorylation of the 20-kD regulatory myosin light chain (MLC20) enhances filament formation. It is not known whether the phosphorylated and non-phosphorylated filaments possess the same structural integrity. We purified myosin from bovine trachealis to form filaments, in ATP-containing zero-calcium solution during a slow ...\n",
      "\n",
      "\n",
      "=== Document rank 3 | id=254592 | similarity=0.705 ===\n",
      "Answer-like snippet:\n",
      "Pharmacological activation of cGMP-dependent protein kinase G I (PKGI) has emerged as a therapeutic strategy for humans with heart failure. However, PKG-activating drugs have been limited by hypotension arising from PKG-induced vasodilation. ...\n",
      "\n",
      "Context (truncated):\n",
      "Pharmacological activation of cGMP-dependent protein kinase G I (PKGI) has emerged as a therapeutic strategy for humans with heart failure. However, PKG-activating drugs have been limited by hypotension arising from PKG-induced vasodilation. PKGIα antiremodeling substrates specific to the myocardium might provide targets to circumvent this limitation, but currently remain poorly understood. We per ...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 1: use a question from our held-out test set\n",
    "example_row = test_df.iloc[0]\n",
    "demo_question = example_row[\"question\"]\n",
    "\n",
    "qa_retrieve(demo_question, top_k_docs=3, snippet_chars=400)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff39479-571b-40f5-a945-13b0229252a5",
   "metadata": {},
   "source": [
    "# Then for a fully live demo, we can also type any question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8215ad33-2a6a-4750-9bb0-ef1f2cf8ec15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION:\n",
      "Does metformin help diabetes?\n",
      "\n",
      "Top retrieved documents:\n",
      "\n",
      "=== Document rank 1 | id=69408 | similarity=0.865 ===\n",
      "Answer-like snippet:\n",
      "To know whether metformin improves postprandial hyperglycaemia, we examined the effect of metformin on the glycated albumin (GA) to glycated haemoglobin (HbA1c) ratio (GA/HbA1c ratio) in patients with newly diagnosed type 2 diabetes. Metformin and lifestyle interventions were initiated in 18 patients with newly diagnosed type 2 diabetes. ...\n",
      "\n",
      "Context (truncated):\n",
      "To know whether metformin improves postprandial hyperglycaemia, we examined the effect of metformin on the glycated albumin (GA) to glycated haemoglobin (HbA1c) ratio (GA/HbA1c ratio) in patients with newly diagnosed type 2 diabetes. Metformin and lifestyle interventions were initiated in 18 patients with newly diagnosed type 2 diabetes. Metformin was titrated to 1500 mg/day or maximum-tolerated d ...\n",
      "\n",
      "\n",
      "=== Document rank 2 | id=174263 | similarity=0.858 ===\n",
      "Answer-like snippet:\n",
      "\"High dose\" metformin therapy (2,550 mg/day) is reported to improve glycemic control in type 2 diabetic patients with obesity (body mass index (BMI) > or = 30). Some have reported that metformin therapy, even in low doses (500-750 mg/day), improves glycemic control in non-obese type 2 diabetic patients (BMI approximately 25). ...\n",
      "\n",
      "Context (truncated):\n",
      "\"High dose\" metformin therapy (2,550 mg/day) is reported to improve glycemic control in type 2 diabetic patients with obesity (body mass index (BMI) > or = 30). Some have reported that metformin therapy, even in low doses (500-750 mg/day), improves glycemic control in non-obese type 2 diabetic patients (BMI approximately 25). However, it is unclear whether \"low dose\" metformin improves glycemic co ...\n",
      "\n",
      "\n",
      "=== Document rank 3 | id=16740 | similarity=0.839 ===\n",
      "Answer-like snippet:\n",
      "While metformin is generally accepted as the first-line agent in treatment of type 2 diabetes, there are insufficient evidence and extensive debate about the best second-line agent. We aimed to assess the benefits and harms of four commonly used antihyperglycemia treatment regimens considering clinical effectiveness, quality of life, and cost. ...\n",
      "\n",
      "Context (truncated):\n",
      "While metformin is generally accepted as the first-line agent in treatment of type 2 diabetes, there are insufficient evidence and extensive debate about the best second-line agent. We aimed to assess the benefits and harms of four commonly used antihyperglycemia treatment regimens considering clinical effectiveness, quality of life, and cost. We developed and validated a new population-based glyc ...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 2: ad-hoc question during presentation\n",
    "custom_question = \"Does metformin help diabetes?\"\n",
    "qa_retrieve(custom_question, top_k_docs=3, snippet_chars=400)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c090c59f-a23f-4e31-9a24-1db7974e351d",
   "metadata": {},
   "source": [
    "# a tiny “ask me anything” loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f68459f2-3b91-43de-b4a5-4485163e9183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type a biomedical question (or just press Enter to quit).\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Q:  does smoking cause heart attack?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION:\n",
      "does smoking cause heart attack?\n",
      "\n",
      "Top retrieved documents:\n",
      "\n",
      "=== Document rank 1 | id=221678 | similarity=0.763 ===\n",
      "Answer-like snippet:\n",
      "Smoking accounts for more than 5 million years of potential life lost per year in the US alone. Leading causes of smoking attributable mortality are acute atherothrombotic complications of coronary heart disease (CHD). ...\n",
      "\n",
      "Context (truncated):\n",
      "Smoking accounts for more than 5 million years of potential life lost per year in the US alone. Leading causes of smoking attributable mortality are acute atherothrombotic complications of coronary heart disease (CHD). Smoking cessation is a key issue in preventive medicine, but quantitative data on its benefit for the coronary arteries are sparse. ...\n",
      "\n",
      "\n",
      "=== Document rank 2 | id=57021 | similarity=0.745 ===\n",
      "Answer-like snippet:\n",
      "To validate self-report about smoking cessation with biochemical markers of smoking activity amongst patients with ischaemic heart disease. Outpatients at the Division of Cardiology, 75 years of age or younger, who had been Hospitalized at Sahlgrenska University Hospital in Göteborg due to an ischaemic event and who consecutively participated in a  ...\n",
      "\n",
      "Context (truncated):\n",
      "To validate self-report about smoking cessation with biochemical markers of smoking activity amongst patients with ischaemic heart disease. Outpatients at the Division of Cardiology, 75 years of age or younger, who had been Hospitalized at Sahlgrenska University Hospital in Göteborg due to an ischaemic event and who consecutively participated in a  ...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Q:  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bye.\n",
      "Gold article rank for this question: 1\n"
     ]
    }
   ],
   "source": [
    "def interactive_qa():\n",
    "    print(\"Type a biomedical question (or just press Enter to quit).\")\n",
    "    while True:\n",
    "        q = input(\"\\nQ: \").strip()\n",
    "        if q == \"\":\n",
    "            print(\"Thank you and see you next time.\")\n",
    "            break\n",
    "        qa_retrieve(q, top_k_docs=2, snippet_chars=350)\n",
    "\n",
    "interactive_qa()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190f31bb-c082-4bd7-9800-a7d25def19d8",
   "metadata": {},
   "source": [
    "# Save everything "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13d0f1b8-fbd9-4baf-9897-81ce505d5a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: df_all_pubmedqa.parquet, context_embs_pubmedqa.npy, dual_encoder_pubmedqa.pt\n"
     ]
    }
   ],
   "source": [
    "# 1) Save the big dataframe with question/context/id\n",
    "# Parquet or pickle is nicer than CSV for large text\n",
    "df_all.to_parquet(\"df_all_pubmedqa.parquet\")  # or: df_all.to_pickle(\"df_all_pubmedqa.pkl\")\n",
    "\n",
    "# 2) Save the context embeddings\n",
    "np.save(\"context_embs_pubmedqa.npy\", context_embs)\n",
    "\n",
    "# 3) Save the fine-tuned dual encoder weights\n",
    "torch.save(dual_encoder.state_dict(), \"dual_encoder_pubmedqa.pt\")\n",
    "\n",
    "print(\"Saved: df_all_pubmedqa.parquet, context_embs_pubmedqa.npy, dual_encoder_pubmedqa.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b962fe-6fa5-4ea5-84a7-e96173a5d85a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbe2301-3f3d-4707-a7c3-98296a251808",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
