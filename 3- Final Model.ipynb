{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc87c21-66ac-4bd9-8c20-aa46950fda49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db4a5ca0-578f-4b66-9490-75ceebbbdae0",
   "metadata": {},
   "source": [
    "### If you want to just demo my QA system (no training, no encoding all contexts again), you can make a new notebook with just only import my already saved trained models as below => "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf61da38-3139-4502-9cb9-33a7b3b64c78",
   "metadata": {},
   "source": [
    "# 1. Imports + device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "efb7e077-bbd1-4476-b2f7-423fc6f6a509",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ce6121af-1c88-4638-80ff-1c5ed8a6ed98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5026a1-77e8-48bc-8d93-c1539adca484",
   "metadata": {},
   "source": [
    "# 2. Reload data and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "94706c4e-836b-4b97-bd77-a32d61a04e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                            question  \\\n",
       " 0  Do mitochondria play a role in remodelling lac...   \n",
       " 1  Landolt C and snellen e acuity: differences in...   \n",
       " 2  Syncope during bathing in infants, a pediatric...   \n",
       " 3  Are the long-term results of the transanal pul...   \n",
       " 4  Can tailored interventions increase mammograph...   \n",
       " \n",
       "                                              context  id  \n",
       " 0  Programmed cell death (PCD) is the regulated d...   0  \n",
       " 1  Assessment of visual acuity depends on the opt...   1  \n",
       " 2  Apparent life-threatening events in infants ar...   2  \n",
       " 3  The transanal endorectal pull-through (TERPT) ...   3  \n",
       " 4  Telephone counseling and tailored print commun...   4  ,\n",
       " (273467, 768))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataframe with question/context/id\n",
    "df_all = pd.read_parquet(\"df_all_pubmedqa.parquet\")   # or read_pickle/read_csv depending on how you saved\n",
    "\n",
    "# Load the context embedding matrix\n",
    "context_embs = np.load(\"context_embs_pubmedqa.npy\")   # shape [num_contexts, hidden_size]\n",
    "\n",
    "df_all.head(), context_embs.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459f1b90-469d-414b-9fdb-f510578d4fee",
   "metadata": {},
   "source": [
    "# 3. Recreate the DualEncoder class + load weights\n",
    "\n",
    "We need the same class definition as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0b7b76d5-31bc-4351-bf3f-1222e9c11823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DualEncoder(\n",
       "  (encoder): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "max_length = 128  # same as during training\n",
    "\n",
    "class DualEncoder(nn.Module):\n",
    "    def __init__(self, encoder_name):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(encoder_name)\n",
    "        self.hidden_size = self.encoder.config.hidden_size\n",
    "\n",
    "    def encode(self, input_ids, attention_mask):\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        token_embs = outputs.last_hidden_state  # [B, L, H]\n",
    "        cls_emb = token_embs[:, 0, :]          # [B, H]\n",
    "        cls_emb = cls_emb / cls_emb.norm(p=2, dim=1, keepdim=True).clamp(min=1e-8)\n",
    "        return cls_emb\n",
    "\n",
    "    def forward(self, q_input_ids, q_attention_mask, c_input_ids, c_attention_mask):\n",
    "        q_emb = self.encode(q_input_ids, q_attention_mask)\n",
    "        c_emb = self.encode(c_input_ids, c_attention_mask)\n",
    "        return q_emb, c_emb\n",
    "\n",
    "dual_encoder = DualEncoder(model_name).to(device)\n",
    "\n",
    "# Load your fine-tuned weights\n",
    "state_dict = torch.load(\"dual_encoder_pubmedqa.pt\", map_location=device)\n",
    "dual_encoder.load_state_dict(state_dict)\n",
    "dual_encoder.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9191b9-0e21-45b0-b63a-fccdc9978b84",
   "metadata": {},
   "source": [
    "# 4. Add a small generative model for plain-language answers\n",
    "\n",
    "This model is just for generation, not training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2c7b84dd-04f0-4950-86e3-cadccb643316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.8.0'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "openai.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b7798ef3-ebf9-4317-b871-2786f372aac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load API key from text file\n",
    "with open(\"key.txt\", \"r\") as f:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = f.read().strip()\n",
    "\n",
    "# Initialize client using the environment variable\n",
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f97b21-57a6-4179-a405-c7950f74ca89",
   "metadata": {},
   "source": [
    "# 5. Spliting retrieval into a function that returns the top docs\n",
    "\n",
    "following retrieve_topk_docs function will return the top contexts for the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0a043c18-12a1-465f-817b-6c2ad4620ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_topk_docs(question, top_k_docs=3):\n",
    "    dual_encoder.eval()\n",
    "\n",
    "    enc = tokenizer(\n",
    "        question,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        q_emb = dual_encoder.encode(\n",
    "            enc[\"input_ids\"].to(device),\n",
    "            enc[\"attention_mask\"].to(device)\n",
    "        ).cpu().numpy()[0]\n",
    "\n",
    "    sims = context_embs @ q_emb\n",
    "    ranked_indices = np.argsort(-sims)\n",
    "\n",
    "    docs = []\n",
    "    for rank_pos in range(top_k_docs):\n",
    "        cid = ranked_indices[rank_pos]\n",
    "        score = sims[cid]\n",
    "        ctx = df_all.loc[df_all[\"id\"] == cid, \"context\"].values[0]\n",
    "        docs.append((score, cid, ctx))\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6485c43-5b24-4f98-af04-e8039957c97f",
   "metadata": {},
   "source": [
    "# 6. Generate a simple English answer from the evidence\n",
    "\n",
    "Now we use the top-k contexts as “evidence” and ask FLAN-T5 to answer the question in lay terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "faadf9a6-55bf-4255-a1b8-5e26f1b3b3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_plain_answer_gpt4o(question, docs, max_chars_per_doc=400):\n",
    "    # Convert retrieved docs into short evidence snippets\n",
    "    snippets = []\n",
    "    for score, cid, ctx in docs:\n",
    "        snippets.append(ctx[:max_chars_per_doc])\n",
    "    evidence_text = \"\\n\\n\".join(snippets)\n",
    "\n",
    "    # SEMI-STRICT RAG PROMPT — grounded, but not too restrictive\n",
    "    prompt = f\"\"\"\n",
    "    You are a biomedical assistant. \n",
    "    You MUST answer using ONLY the evidence below.\n",
    "    However, you ARE allowed to:\n",
    "    - infer likely meaning if multiple snippets point in the same direction,\n",
    "    - paraphrase or summarize what the evidence implies,\n",
    "    - generalize a little ONLY if the evidence strongly suggests it.\n",
    "    \n",
    "    You are NOT allowed to:\n",
    "    - use outside medical knowledge,\n",
    "    - add unsupported facts,\n",
    "    - contradict the evidence.\n",
    "    \n",
    "    Question:\n",
    "    {question}\n",
    "    \n",
    "    Evidence:\n",
    "    {evidence_text}\n",
    "    \n",
    "    OUTPUT RULES:\n",
    "    1. Start with EXACTLY one of:\n",
    "       - Short answer: Yes.\n",
    "       - Short answer: No.\n",
    "       - Short answer: It leans toward yes.\n",
    "       - Short answer: It leans toward no.\n",
    "       - Short answer: Unclear.\n",
    "    \n",
    "    2. Then add 1–2 simple sentences summarizing what the evidence suggests.\n",
    "    3. If evidence is indirect, incomplete, or off-topic -> choose \"Short answer: Unclear.\"\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0  # no creativity → no hallucination\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73e24cf-e639-4c61-b14b-e24da8daa4b4",
   "metadata": {},
   "source": [
    "# 7. A user-facing question answering system:\n",
    "\n",
    "in the following it will define a high-level function that:\n",
    "    \n",
    "    * Retrieves top-k docs\n",
    "    * Generates a simple answer\n",
    "    * Shows both the answer and the evidence snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1dcbeb28-2cfe-4367-b33f-56324e177f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_explained(question, top_k_docs=3, snippet_chars=350):\n",
    "    # 1) retrieve top docs\n",
    "    docs = retrieve_topk_docs(question, top_k_docs=top_k_docs)\n",
    "\n",
    "    print(\"QUESTION:\")\n",
    "    print(question)\n",
    "    print(\"\\nSHORT ANSWER (for non-medical audience):\\n\")\n",
    "\n",
    "    # 2) generate simple explanation\n",
    "    plain_answer = generate_plain_answer_gpt4o(question, docs)\n",
    "    print(plain_answer)\n",
    "    print(\"\\n---\\nEVIDENCE SNIPPETS FROM ARTICLES:\\n\")\n",
    "\n",
    "    # 3) show evidence\n",
    "    for rank_pos, (score, cid, ctx) in enumerate(docs, start=1):\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', ctx)\n",
    "        sentences = [s.strip() for s in sentences if len(s.strip()) > 0]\n",
    "\n",
    "        print(f\"=== Document rank {rank_pos} | id={cid} | similarity={score:.3f} ===\")\n",
    "        print(\"Snippet:\")\n",
    "        print(\" \".join(sentences[:2])[:snippet_chars], \"...\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04155e11-4957-4874-ba09-45f80e094732",
   "metadata": {},
   "source": [
    "# 8. Interactive loop using the friendly QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed69f8f1-4422-4368-9ed7-5014a8747243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type a biomedical question (or just press Enter to quit).\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Q:  is pregnancy cause hair loss?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION:\n",
      "is pregnancy cause hair loss?\n",
      "\n",
      "SHORT ANSWER (for non-medical audience):\n",
      "\n",
      "Short answer: Unclear.  \n",
      "The evidence provided does not directly address the relationship between pregnancy and hair loss. It discusses hair loss in the context of androgenetic alopecia and other factors but does not mention pregnancy specifically.\n",
      "\n",
      "---\n",
      "EVIDENCE SNIPPETS FROM ARTICLES:\n",
      "\n",
      "=== Document rank 1 | id=81143 | similarity=0.702 ===\n",
      "Snippet:\n",
      "Anecdotal reports suggest that certain disorders are common in African hair and may be associated with hairstyles. A cross-sectional study of 1042 schoolchildren was performed to test this hypothesis. ...\n",
      "\n",
      "=== Document rank 2 | id=36509 | similarity=0.643 ===\n",
      "Snippet:\n",
      "The aim of this study was to evaluate the effects of androgenetic alopecia on males with and without hair loss and to delineate the level of stress gained by this type of alopecia. Two hundred and 52 males (175 with hair loss, 77 without hair loss), between 16 and 72 years of age, participated in the study. ...\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Q:  is smoking bad?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION:\n",
      "is smoking bad?\n",
      "\n",
      "SHORT ANSWER (for non-medical audience):\n",
      "\n",
      "Short answer: Yes.  \n",
      "The evidence indicates that tobacco use is a significant public health issue, particularly among Chinese males, suggesting that smoking is associated with various health risks. Additionally, smoking behaviors may be influenced by social acceptance, further highlighting its negative implications.\n",
      "\n",
      "---\n",
      "EVIDENCE SNIPPETS FROM ARTICLES:\n",
      "\n",
      "=== Document rank 1 | id=214442 | similarity=0.858 ===\n",
      "Snippet:\n",
      "According to a recent national survey, tobacco use is a critical public health issue in China, with more than two-thirds of Chinese males smoking. Findings in Western populations suggest that smoking may cluster with other health-risk behaviors. ...\n",
      "\n",
      "=== Document rank 2 | id=230993 | similarity=0.857 ===\n",
      "Snippet:\n",
      "Nearly all studies reporting smoking status collect self-reported data. The objective of this study was to assess sociodemographic characteristics and selected, common smoking-related diseases as predictors of invalid reporting of non-smoking. ...\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Q:  is eating fast food make us fat?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION:\n",
      "is eating fast food make us fat?\n",
      "\n",
      "SHORT ANSWER (for non-medical audience):\n",
      "\n",
      "Short answer: Unclear.  \n",
      "The evidence indicates that while fast food consumption is linked to adverse health outcomes, the specific contribution of fast food to overweight or obesity compared to other dietary patterns is not clearly established.\n",
      "\n",
      "---\n",
      "EVIDENCE SNIPPETS FROM ARTICLES:\n",
      "\n",
      "=== Document rank 1 | id=13977 | similarity=0.864 ===\n",
      "Snippet:\n",
      "Although fast food consumption has been linked to adverse health outcomes, the relative contribution of fast food itself compared with the rest of the diet to these associations remains unclear. Our objective was to compare the independent associations with overweight/obesity or dietary outcomes for fast food consumption compared with dietary patte ...\n",
      "\n",
      "=== Document rank 2 | id=88460 | similarity=0.822 ===\n",
      "Snippet:\n",
      "To examine associations of the frequency of eating at fast-food restaurants with demographic, behavioural and psychosocial factors and dietary intake in African American adults. Self-reported data from a population-based cross-sectional survey of 658 African Americans, aged 20-70 years, in North Carolina. ...\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Q:  is eating mercury harmful?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION:\n",
      "is eating mercury harmful?\n",
      "\n",
      "SHORT ANSWER (for non-medical audience):\n",
      "\n",
      "Short answer: Yes.  \n",
      "The evidence indicates that high mercury content in fish may negate the cardiovascular benefits of consuming fish, suggesting that mercury exposure can be harmful. Additionally, methylmercury exposure is primarily linked to fish consumption, highlighting the risks associated with mercury in the environment.\n",
      "\n",
      "---\n",
      "EVIDENCE SNIPPETS FROM ARTICLES:\n",
      "\n",
      "=== Document rank 1 | id=201691 | similarity=0.711 ===\n",
      "Snippet:\n",
      "Fish consumption is considered the primary pathway of methylmercury (MeHg) exposure for most people in the world. However, in the inland regions of China, most of the residents eat little fish, but they live in areas where a significant amount of mercury (Hg) is present in the environment. ...\n",
      "\n",
      "=== Document rank 2 | id=52513 | similarity=0.694 ===\n",
      "Snippet:\n",
      "Fish consumption has been associated with a decreased risk of coronary artery disease. Recent studies have illustrated that the high mercury content in cold-water fish may negate the cardiovascular benefits of fish meals. ...\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Q:  is Charles the king of England now?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION:\n",
      "is Charles the king of England now?\n",
      "\n",
      "SHORT ANSWER (for non-medical audience):\n",
      "\n",
      "Short answer: Unclear.  \n",
      "The provided evidence does not contain any information regarding the current status of Charles as the king of England.\n",
      "\n",
      "---\n",
      "EVIDENCE SNIPPETS FROM ARTICLES:\n",
      "\n",
      "=== Document rank 1 | id=65409 | similarity=0.575 ===\n",
      "Snippet:\n",
      "Determining the reasons people choose to study nursing may help educators and managers develop student-focussed and enticing nursing programmes. In Australia, little research has been undertaken with students entering nursing programmes and the reasons for their choice. ...\n",
      "\n",
      "=== Document rank 2 | id=153117 | similarity=0.574 ===\n",
      "Snippet:\n",
      "Between roughly 500 BCE and 300 BCE, three distinct regions, the Yangtze and Yellow River Valleys, the Eastern Mediterranean, and the Ganges Valley, saw the emergence of highly similar religious traditions with an unprecedented emphasis on self-discipline and asceticism and with \"otherworldly,\" often moralizing, doctrines, including Buddhism, Jaini ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def interactive_qa_simple():\n",
    "    print(\"Type a biomedical question (or just press Enter to quit).\")\n",
    "    while True:\n",
    "        q = input(\"\\nQ: \").strip()\n",
    "        if q == \"\":\n",
    "            print(\"Thank you and see you next time.\")\n",
    "            break\n",
    "        qa_explained(q, top_k_docs=2, snippet_chars=350)\n",
    "\n",
    "# When you’re ready to demo:\n",
    "interactive_qa_simple()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297a4d32-79d3-42dd-98fe-f841d8edaa0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4700b2-a6bf-4450-90f1-323be04e2089",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7887f9-74a7-43ca-b490-61ab9919681e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
